% Optimizado para compilación rápida
\documentclass[paper=letter, fontsize=11pt, draft=false]{scrartcl}

% Modifications to layout
\usepackage[shortlabels]{enumitem} % Incisos
\def\code#1{\texttt{#1}} % \code{} for monospaced text
\newcommand{\RNum}[1]{\footnotesize\uppercase\expandafter{\romannumeral #1\relax\normalsize}} % Roman numbers

\usepackage{subcaption} % 2x2 graphs
\usepackage{mwe}
\usepackage{float} % [H] in graphics
\usepackage[hidelinks]{hyperref}  % Hipervínculos en la TOC

\usepackage{booktabs,siunitx,listings}
\usepackage[most]{tcolorbox}
\tcbuselibrary{theorems}
\usepackage{cleveref}

% Typography and layout packages
\usepackage{graphicx}
\usepackage{verbatim}
% \usepackage{xcolor}
\usepackage[spanish,es-nodecimaldot]{babel} % Language and hyphenation
\usepackage{amsmath, amsfonts, amsthm, amssymb} % Math packages
\newtheorem{definition}{Definición} % definition
\usepackage{fancyvrb}
\usepackage{sectsty} % Customize section commands
\usepackage{geometry} % Modify margins
\usepackage{titlesec} % Customize section titles
\geometry{margin=3cm,top=2.5cm,bottom=2.5cm} % Simplified geometry
\allsectionsfont{\centering \normalfont\scshape} % Center and style section titles

% Header and footer customization
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead[L]{\slshape} % Remove section title from header
\fancyhead[C]{} % Header center
\fancyhead[R]{\thepage} % Header right with page number
\fancyfoot[L]{} % Footer left
\fancyfoot[C]{} % Footer center
\fancyfoot[R]{\small \slshape Gustavo Hernández Angeles} % Footer right
\renewcommand{\headrulewidth}{0.4pt} % Header rule
\renewcommand{\footrulewidth}{0.4pt} % Footer rule
\setlength{\headheight}{14.5pt} % Header height

% Paragraph settings
\setlength\parindent{0pt}
\setlength{\parskip}{1ex}

% Section spacing
\titlespacing*{\section}{0cm}{0.50cm}{0.25cm}

% --- Theorems, lemma, corollary, postulate, definition ---
\definecolor{Pantone209C}{HTML}{64293e}

\newcounter{problemcounter}

\numberwithin{equation}{problemcounter} % Number equations within problems
\numberwithin{figure}{problemcounter} % Number figures within problems
\numberwithin{table}{problemcounter} % Number tables within problems
\numberwithin{subsection}{problemcounter} 

\newtcbtheorem[auto counter]{problem}{Ejercicio}{
    enhanced,
    breakable,
    colback = gray!5,
    colframe = gray!5,
    boxrule = 0.5pt,
    sharp corners,
    borderline west = {2mm}{0mm}{Pantone209C},
    fonttitle = \bfseries\sffamily,
    coltitle = Pantone209C,
    drop fuzzy shadow,
    parbox = false,
    before skip = 3ex,
    after skip = 3ex
}{problem}
\makeatletter
\renewenvironment{problem}[2][]{%
    \refstepcounter{problemcounter}%
    \addcontentsline{toc}{section}{\protect\numberline{\theproblemcounter}Ejercicio \theproblemcounter: #2}%
    \begin{tcolorbox}[
        enhanced,
        breakable,
        colback = gray!5,
        colframe = gray!5,
        boxrule = 0.5pt,
        sharp corners,
        borderline west = {2mm}{0mm}{Pantone209C},
        fonttitle = \bfseries\sffamily,
        coltitle = Pantone209C,
        drop fuzzy shadow,
        parbox = false,
        before skip = 3ex,
        after skip = 3ex,
        title={Ejercicio \theproblemcounter: #2}
    ]
}{%
    \end{tcolorbox}
}
\makeatother

\tcbuselibrary{skins, breakable}
% Custom command for a horizontal rule
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 

% Custom section titles with numbering
\titleformat{\section}
{\normalfont\Large\bfseries}{\thesection}{1em}{}

\titleformat{\subsection}
{\normalfont\large\bfseries}{\thesubsection}{1em}{}

\titleformat{\subsubsection}
{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

% Title and author
\title{	
    \begin{center}
        \includegraphics[width=3cm]{figure/template/cimat-logo.png} % Adjust size as needed
    \end{center}
    \vspace{0.5cm}
    \normalfont \normalsize 
    \textbf{\Large   Centro de Investigación en Matemáticas} \\
    \Large Unidad Monterrey \\ [25pt] 
    \horrule{1pt} \\[0.4cm] % Thin top horizontal rule
    \huge Análisis Multimodal\\
    \Large Tarea 2\\ 
    \horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{\large Gustavo Hernández Angeles}    

\date{\normalsize\today} % Today's date

\begin{document}
\maketitle % Print the title
\thispagestyle{empty}
\newpage

\tableofcontents
























































%%%%%%%%%%% PROBLEMA 1 %%%%%%%%%%%
\newpage
\begin{problem}{}
Utilizando el conjunto de datos \texttt{College} disponible en la libreria \texttt{ISLR}, predice el número de solicitudes recibidas (\texttt{Apps}) utilizando las otras variables del conjunto de datos.

\begin{enumerate}[a)]
  \item Divide el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba.
  \item Ajusta un modelo lineal utilizando mínimos cuadrados en el conjunto de entrenamiento y reporta el error de prueba obtenido.
  \item Ajusta un modelo de regresión ridge en el conjunto de entrenamiento, con \(\lambda\) elegido por validación cruzada. Reporta el error de prueba obtenido.
  \item Ajusta un modelo Lasso en el conjunto de entrenamiento, con \(\lambda\) elegido por validación cruzada. Reporta el error de prueba obtenido, junto con el número de estimaciones de coeficientes distintos de cero.
  \item Ajusta un modelo PCR en el conjunto de entrenamiento, con M elegido por validación cruzada. Reporta el error de prueba obtenido, junto con el valor de M seleccionado por validación cruzada.
  \item Ajusta un modelo PLS en el conjunto de entrenamiento, con M elegido por validación cruzada. Reporta el error de prueba obtenido, junto con el valor de M seleccionado por validación cruzada.
  \item Comenta los resultados obtenidos. ¿Con qué precisión podemos predecir la cantidad de solicitudes universitarias recibidas?  ¿Hay mucha diferencia entre los errores de prueba resultantes de estos cinco enfoques? 
  \item Propón un modelo (o un conjunto de modelos) que parezca funcionar bien en este conjunto de datos y justifica tu respuesta. Asegúrate de evaluar el rendimiento del modelo utilizando el error del conjunto de validación, la validación cruzada o alguna otra alternativa razonable, en lugar de utilizar el error de entrenamiento. ¿El modelo que elegiste incluye todas las características del conjunto de datos? ¿Por qué o por qué no? 
\end{enumerate}

\end{problem}

\subsection{Inciso a)}

Se utilizó el conjunto de datos \texttt{College} y, para evitar data leakage, se eliminaron las variables \texttt{Accept} y \texttt{Enroll} del análisis, ya que están determinadas después de \texttt{Apps} o están fuertemente condicionadas por ella; incluirlas inflaría artificialmente el desempeño en prueba. Posteriormente, se dividió el conjunto en entrenamiento y prueba usando un \(50\%\) para cada uno (muestra aleatoria con semilla fija).

\subsection{Inciso b)}

Se ajustó un modelo lineal utilizando Minimos Cuadrados Ordinarios (OLS) en el conjunto de entrenamiento, empleando todos los predictores. Las variables que resultaron ser estadísticamente significativas (con $p < 0.1$) en dicho modelo fueron:
\begin{itemize}
    \item \texttt{F.Undergrad} ($***$)
    \item \texttt{Room.Board} ($***$)
    \item \texttt{Expend} ($**$)
    \item \texttt{Grad.Rate} ($*$)
    \item \texttt{perc.alumni} ($.$)
\end{itemize}

mientras que las demás variables no mostraron significancia estadística en el modelo ajustado. Al evaluar el rendimiento de este modelo en el conjunto de prueba, se obtuvo un Error Cuadrático Medio (MSE) de \textbf{2,551,734}. Esto corresponde a un Error Cuadrático Medio Raíz (RMSE) de \textbf{1,597.4}.

\subsection{Inciso c)}

Se ajustó una regresión \textit{Ridge} con validación cruzada de 5 pliegues para seleccionar el parámetro de regularización \(\lambda\). El valor de $\lambda$ que minimiza el error \(\lambda_{\min}\) elegido por CV fue \textbf{12.07}, y \(\lambda_{1se}=\) \textbf{4498.43} como alternativa más parsimoniosa (ver Figura \ref{fig:lambda_selection}). Con \(\lambda_{\min}\), el desempeño en prueba fue: MSE \textbf{2,530,947} (RMSE \textbf{1,590.9}). Con \(\lambda_{1se}\) el error aumentó a MSE \textbf{3,552,978} (RMSE \textbf{1,884.9}).

\subsection{Inciso d)}

Se ajustó un modelo \textit{LASSO} también con validación cruzada de 5 pliegues. Se obtuvo \(\lambda_{\min}=\) \textbf{86.85} y \(\lambda_{1se}=\) \textbf{1098.54}. Con \(\lambda_{\min}\), el error de prueba fue MSE \textbf{2,395,665} (RMSE \textbf{1,547.8}), y con \(\lambda_{1se}\) el MSE fue \textbf{3,438,634}. El modelo con \(\lambda_{\min}\) seleccionó 9 coeficientes distintos de cero, correspondientes a las variables: \texttt{Top10perc}, \texttt{Top25perc}, \texttt{F.Undergrad}, \texttt{Room.Board}, \texttt{Personal}, \texttt{S.F.Ratio}, \texttt{perc.alumni}, \texttt{Expend} y \texttt{Grad.Rate}.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/lambda_ridge.png}
        \caption{Selección de $\lambda$ por validación cruzada – Ridge.}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/lambda_lasso.png}
        \caption{Selección de $\lambda$ por validación cruzada – Lasso.}
    \end{subfigure}
    \caption{Comparación de curvas de validación cruzada para la selección de $\lambda$. Las líneas verticales indican $\lambda_{\min}$ y $\lambda_{1se}$.}
    \label{fig:lambda_selection}
\end{figure}

\subsection{Inciso e)}

Para PCR (componentes principales como regresores), con escalamiento y validación cruzada, el menor MSEP se obtuvo con \(M=\) 12 componentes. En prueba, el desempeño fue MSE \textbf{2,389,249} (RMSE \textbf{1,545.7}). Nótese que con 6 componentes se explica aproximadamente el 80.8\% de la varianza en $\mathbf{X}$ y 61.7\% de \texttt{Apps}; con 9 componentes, 91.1\% en $\mathbf{X}$ y 62.7\% en \texttt{Apps}. Como se espera, aumentar el número de componentes no mejora la explicación de \texttt{Apps} significativamente.

\subsection{Inciso f)}

Para PLSR (componentes latentes que maximizan covarianza), la validación cruzada sugirió \(M=\) \textbf{4} componentes. En el conjunto de prueba se obtuvo MSE \textbf{2,457,676} (RMSE \textbf{1,567.7}).

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/m_pcr.png}
        \caption{Curva de MSEP para PCR.}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/m_pls.png}
        \caption{Curva de MSEP para PLSR.}
    \end{subfigure}
    \caption{Selección del número de componentes $M$ mediante validación cruzada.}
\end{figure}

En esta ocasión, el modelo con 4 componentes explica el 72.01\% de la varianza en \texttt{Apps}, lo que indica que PLSR logra una mejor explicación de la variable respuesta con menos componentes en comparación con PCR.



\subsection{Inciso g)}

Para entender el desempeño de los modelos, debemos entender la variable de respuesta \texttt{Apps}. Esta variable tiene un rango amplio, desde un mínimo de 81 hasta un máximo de 48,094 solicitudes, con un promedio de aproximadamente 3,002 y una mediana de 1,558. La distribución es altamente sesgada a la derecha, con algunas universidades recibiendo un número excepcionalmente alto de solicitudes.

% Resumen de errores de prueba
\begin{table}[H]
    \centering
    \caption{Errores de prueba por modelo}
    \label{tab:errores-modelos}
    \begin{tabular}{lrr}
        \toprule
        \textbf{Modelo} & \textbf{MSE} & \textbf{RMSE} \\
        \midrule
        OLS & 2,551,734 & 1,597.4 \\
        Ridge ($\lambda_{\min}=12.07$) & 2,530,947 & 1,590.9 \\
        Lasso ($\lambda_{\min}=86.85$) & 2,395,665 & 1,547.8 \\
        PCR ($M=12$) & \textbf{2,389,249} & \textbf{1,545.7} \\
        PLSR ($M=4$) & 2,457,676 & 1,567.7 \\
        \bottomrule
    \end{tabular}
\end{table}

En términos de precisión, todos los métodos logran errores de prueba de magnitud similar: RMSE entre \(\approx\) 1545 y 1600 solicitudes. Dado que el promedio de \texttt{Apps} ronda \(\sim\) $3000$, el error típico es del orden de la mitad del promedio, lo que indica capacidad predictiva moderada pero con variabilidad sustancial no explicada. Entre los enfoques (Cuadro \ref{tab:errores-modelos}), \textbf{PCR (M=12)} y \textbf{Lasso} con \(\lambda_{\min}\) fueron los mejores (MSE \(\approx 2.39\times10^6\)), seguidos de \textbf{Ridge} (ligeramente peor) y \textbf{PLSR}. El modelo lineal MCO quedó rezagado respecto a PCR/Lasso, aunque no por un margen muy grande.

\subsection{Inciso h)}

Una propuesta razonable es utilizar \textbf{Lasso con \(\lambda_{\min}\)}: ofrece un desempeño competitivo (prácticamente el mejor MSE) y además un modelo \textit{parco} con solo 9 predictores, lo que facilita interpretación y despliegue. Usar \(\lambda_{1se}\) reduciría aún más la complejidad, pero en este caso incurre en una pérdida de precisión considerable. Como alternativa si la interpretabilidad de coeficientes es secundaria, \textbf{PCR con \(M=12\)} proporciona un error prácticamente indistinguible del de Lasso.

El conjunto de covariables retenido por Lasso incluye factores plausibles desde el punto de vista sustantivo (tamaño de matrícula, cuotas, gasto institucional y tasa de graduación), lo que respalda su uso en la práctica.


























































































%%%%%%%%%%% PROBLEMA 2 %%%%%%%%%%%
\newpage
\begin{problem}{}
Es bien sabido que la regresión ridge tiende a dar valores de coeficientes similares a las variables correlacionadas, mientras que lasso puede dar valores de coeficientes totalmente diferentes a las variables correlacionadas. Se explorará esta propiedad en un entorno sencillo.

Supongamos que \(n=2\), \(p=2\), \(x_{11}=x_{12}\), \(x_{21}=x_{22}\). Además, supongamos que \(y_{1}+y_{2}=0\), \(x_{11}+x_{21}=0\) y \(x_{12}+x_{22}=0\), de modo que la estimación del intercepto en mínimos cuadrados, regresión de Ridge o en el modelo de lasso es cero: \(\hat{\gamma}_{0}=0\).

\begin{enumerate}[a)]
  \item Plantea el problema de la optimización con la regresión ridge bajo estas suposiciones.
  \item Argumenta que bajo estas suposiciones, las estimaciones de los coeficientes de ridge satisfacen \(\hat{\beta}_{1}=\hat{\beta}_{2}\).
  \item Plantea el problema de la optimización con la regresión lasso bajo estas suposiciones.
  \item Argumenta que en este contexto, los coeficientes de lasso \(\hat{\beta}_{1}\) y \(\hat{\beta}_{2}\) no son únicos; es decir, hay muchas soluciones posibles al problema de optimización en (c). Describe estas soluciones.
\end{enumerate}

\end{problem}

Primero, analicemos las condiciones dadas:

\begin{itemize}

\item $n=2, p=2$.

\item $x_{11} = x_{12}$ y $x_{21} = x_{22}$. Esto significa que la columna 1 ($X_1$) y la columna 2 ($X_2$) de la matriz $X$ son idénticas: $X_1 = X_2$. Estamos en un caso de colinealidad perfecta.

\item $y_1 + y_2 = 0$ y $x_{11} + x_{21} = 0$. Dado que $\hat{\gamma}_0 = 0$, esto implica que las variables ($y$, $X_1$, $X_2$) están centradas.

\item Para simplificar, definamos $a = x_{11} = x_{12}$ y $c = y_1$.

\item De las condiciones:

\begin{itemize}

\item $x_{21} = -x_{11} = -a$

\item $x_{22} = -x_{12} = -a$ (consistente con $x_{21} = x_{22}$)

\item $y_2 = -y_1 = -c$

\end{itemize}

\end{itemize}

Nuestros datos son:

$$y = \begin{pmatrix} c \\ -c \end{pmatrix}, \quad X = \begin{pmatrix} a & a \\ -a & -a \end{pmatrix}$$

La suma de cuadrados residuales (RSS) es:

$$RSS(\beta_1, \beta_2) = \sum_{i=1}^{2} (y_i - x_{i1}\beta_1 - x_{i2}\beta_2)^2$$

$$RSS = (y_1 - (x_{11}\beta_1 + x_{12}\beta_2))^2 + (y_2 - (x_{21}\beta_1 + x_{22}\beta_2))^2$$

Sustituyendo nuestros valores:

\begin{align*}
    RSS &= (c - (a\beta_1 + a\beta_2))^2 + (-c - (-a\beta_1 - a\beta_2))^2 \\
    RSS &= (c - a(\beta_1 + \beta_2))^2 + (-c + a(\beta_1 + \beta_2))^2 \\
    RSS &= (c - a(\beta_1 + \beta_2))^2 + (-(c - a(\beta_1 + \beta_2)))^2 \\
    RSS &= 2(c - a(\beta_1 + \beta_2))^2
\end{align*}


Como podemos ver, el RSS solo depende de la suma de los coeficientes, $S = \beta_1 + \beta_2$.

\subsection{Inciso a)}

La regresión ridge busca minimizar el RSS más una penalización L2:

$$\min_{\beta} \left\{ \sum_{i=1}^{n} (y_i - x_i^T \beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\}$$

Bajo las suposiciones dadas y con $\hat{\gamma}_0=0$, el problema de optimización para $\beta_1$ y $\beta_2$ es:

$$\min_{\beta_1, \beta_2} \left\{ (y_1 - x_{11}\beta_1 - x_{12}\beta_2)^2 + (y_2 - x_{21}\beta_1 - x_{22}\beta_2)^2 + \lambda (\beta_1^2 + \beta_2^2) \right\}$$

Sustituyendo la forma simplificada del RSS que encontramos:

$$\min_{\beta_1, \beta_2} \left\{ 2(c - a(\beta_1 + \beta_2))^2 + \lambda (\beta_1^2 + \beta_2^2) \right\}$$

(Donde $c=y_1$ y $a=x_{11}$).

\subsection{Inciso b)}

El argumento se basa en la \textbf{simetría} de la función objetivo de ridge y la \textbf{unicidad} de la penalización L2.

\begin{enumerate}

\item \textbf{Simetría:} Sea la función objetivo $L_{Ridge}(\beta_1, \beta_2) = 2(c - a(\beta_1 + \beta_2))^2 + \lambda (\beta_1^2 + \beta_2^2)$.

\begin{itemize}

\item El término RSS, $2(c - a(\beta_1 + \beta_2))^2$, es simétrico respecto a $\beta_1$ y $\beta_2$. Si los intercambiamos, el valor no cambia.

\item El término de penalización, $\lambda (\beta_1^2 + \beta_2^2)$, también es simétrico.

\item Por lo tanto, la función objetivo total $L_{Ridge}(\beta_1, \beta_2)$ es simétrica.

\end{itemize}

\item \textbf{Unicidad del Mínimo:} Para $\lambda > 0$, la función $L_{Ridge}$ es estrictamente convexa. Esto se debe a que la penalización L2 (una esfera) es estrictamente convexa. Esto garantiza que existe un \textit{único} mínimo global $(\hat{\beta}_1, \hat{\beta}_2)$.

\end{enumerate}

Dado que la función es simétrica y tiene un único mínimo, el minimizador en sí debe ser simétrico, lo que implica $\hat{\beta}_1 = \hat{\beta}_2$.

\subsection{Inciso c)}

La regresión lasso busca minimizar el RSS más una penalización L1:

$$\min_{\beta} \left\{ \sum_{i=1}^{n} (y_i - x_i^T \beta)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}$$

Bajo las suposiciones dadas:

$$\min_{\beta_1, \beta_2} \left\{ (y_1 - x_{11}\beta_1 - x_{12}\beta_2)^2 + (y_2 - x_{21}\beta_1 - x_{22}\beta_2)^2 + \lambda (|\beta_1| + |\beta_2|) \right\}$$

Sustituyendo la forma simplificada del RSS:

$$\min_{\beta_1, \beta_2} \left\{ 2(c - a(\beta_1 + \beta_2))^2 + \lambda (|\beta_1| + |\beta_2|) \right\}$$

\subsection{Inciso d)}

A diferencia de Ridge, la penalización L1 no produce un mínimo único en este escenario. El argumento es el siguiente:

\begin{enumerate}

\item \textbf{Descomposición del Problema:} Al igual que con Ridge, podemos descomponer el problema. Sea $S = \beta_1 + \beta_2$.

$$\min_{S} \left\{ 2(c - aS)^2 + \min_{\beta_1+\beta_2=S} \left( \lambda (|\beta_1| + |\beta_2|) \right) \right\}$$

\item \textbf{Problema Interno (Penalización L1):} Consideremos el problema interno $\min (|\beta_1| + |\beta_2|)$ sujeto a $\beta_1+\beta_2=S$.

\begin{itemize}

    \item El valor mínimo de esta penalización es $\lambda |S|$.

    \item Sin embargo, este mínimo \textbf{no es único}. Se alcanza para \textit{cualquier} par $(\beta_1, \beta_2)$ tal que $\beta_1 + \beta_2 = S$ y $\beta_1 \cdot \beta_2 \ge 0$ (es decir, $\beta_1$ y $\beta_2$ tienen el mismo signo o uno es cero).

    \item Por ejemplo, si $S=5$, el mínimo de penalización (5) se alcanza en $(5, 0)$, $(0, 5)$, $(2, 3)$, $(4, 1)$, etc.

\end{itemize}



\item \textbf{Problema Externo (Encontrar $\hat{S}$):} El problema se reduce a encontrar la \textit{suma} óptima $\hat{S}$ minimizando:

$$\min_{S} \left\{ 2(c - aS)^2 + \lambda |S| \right\}$$

Este es un problema de optimización 1D (un "Lasso 1D") que tiene una solución \textbf{única} para la suma, $\hat{S}$. Esta solución $\hat{S}$ es el resultado de aplicar \textit{soft-thresholding} a la solución OLS ($S_{OLS} = c/a$).



\item \textbf{Descripción de las Soluciones:}

Una vez que se encuentra el valor único $\hat{S}$, la solución $(\hat{\beta}_1, \hat{\beta}_2)$ es \textit{cualquier} par que cumpla:

\begin{enumerate}

    \item $\hat{\beta}_1 + \hat{\beta}_2 = \hat{S}$

    \item $|\hat{\beta}_1| + |\hat{\beta}_2| = |\hat{S}|$

\end{enumerate}



Esto describe un conjunto de soluciones (un cojunto convexo):

\begin{itemize}

    \item \textbf{Si $\hat{S} > 0$:} El conjunto de soluciones es el segmento de línea que conecta $(\hat{S}, 0)$ y $(0, \hat{S})$. Es decir, todos los puntos $(\beta_1, \hat{S}-\beta_1)$ para $\beta_1 \in [0, \hat{S}]$.

    \item \textbf{Si $\hat{S} < 0$:} El conjunto de soluciones es el segmento de línea que conecta $(\hat{S}, 0)$ y $(0, \hat{S})$. Es decir, todos los puntos $(\beta_1, \hat{S}-\beta_1)$ para $\beta_1 \in [\hat{S}, 0]$.

    \item \textbf{Si $\hat{S} = 0$:} (Es decir, si $\lambda$ es suficientemente grande), la solución es única: $(\hat{\beta}_1, \hat{\beta}_2) = (0, 0)$.

\end{itemize}

\end{enumerate}

En resumen, mientras que Ridge (L2) identifica la solución simétrica $\hat{\beta}_1 = \hat{\beta}_2$ como la única óptima para una suma dada, Lasso (L1) considera que \textit{todas} las soluciones en el mismo signo (incluyendo las soluciones \textit{sparse} como $(\hat{S}, 0)$ o $(0, \hat{S})$) son igualmente óptimas.



\end{document}
