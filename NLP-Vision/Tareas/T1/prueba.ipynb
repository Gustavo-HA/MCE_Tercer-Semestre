{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cad7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032aca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/meia_data.csv\")\n",
    "data = data.drop(columns=[\"Town\",\"Region\",\"Type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a2ef7a",
   "metadata": {},
   "source": [
    "## 1. Descripcion del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8403027",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos = data[\"Review\"].tolist()\n",
    "corpus = \" \".join(documentos)\n",
    "n = len(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd096f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-zA-ZáéíóúñÁÉÍÓÚÑ]+')\n",
    "doc_tokens = [tokenizer.tokenize(doc) for doc in documentos]\n",
    "tokens = [token for token in tokenizer.tokenize(corpus)]\n",
    "vocab = set(tokens)\n",
    "print(f\"Numero de documentos: {n}\")\n",
    "print(f\"Numero de tokens en el corpus: {len(tokens)}\")\n",
    "print(f\"Numero de palabras en el vocabulario: {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 2\n",
    "# Hapax legomena y su proporcion.\n",
    "fdist = FreqDist(tokens)\n",
    "hapax_legomena = [word for word, freq in fdist.items() if freq == 1]\n",
    "print(f\"Numero de hapax legomena: {len(hapax_legomena)}\")\n",
    "print(f\"Proporcion de hapax legomena con el vocabulario: {len(hapax_legomena) / len(vocab) * 100 if vocab else 0:.3f}%\")\n",
    "for hapax in hapax_legomena:\n",
    "    print(f\" - {hapax}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e867ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parte 3. Porcentaje de stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "spanish_stopwords = set(stopwords.words('spanish'))\n",
    "\n",
    "contador_stopwords = 0\n",
    "\n",
    "for token in tokens:\n",
    "    if token.lower() in spanish_stopwords:\n",
    "        contador_stopwords += 1\n",
    "\n",
    "print(f\"Porcentaje de stopwords: {contador_stopwords / len(tokens) * 100 if tokens else 0:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parte 4. Estadisticas por clase\n",
    "for polarity in sorted(data['Polarity'].unique()):\n",
    "    df_reducido = data.loc[data['Polarity'] == polarity, 'Review'].tolist()\n",
    "    corpus_reducido = ' '.join(df_reducido)\n",
    "    n = len(df_reducido)\n",
    "    tokens_r = tokenizer.tokenize(corpus_reducido)\n",
    "    vocab_r = set(tokens_r)\n",
    "\n",
    "    print(f\"Estadisticas para la polaridad {polarity}:\")\n",
    "    print(f\" - Cantidad de textos: {n}\")\n",
    "    print(f\" - Cantidad de palabras: {len(tokens_r)}\")\n",
    "    print(f\" - Cantidad de palabras en el vocabulario: {len(vocab_r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ef563",
   "metadata": {},
   "source": [
    "## 2. Ley de Zipf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3019e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calcular la frecuencia absoluta de cada palabra en el corpus y asignar rango\n",
    "palabras = []\n",
    "rango = list(range(1, len(fdist) + 1))\n",
    "for palabra, _ in sorted(fdist.items(), key=lambda item: item[1], reverse=True):\n",
    "    palabras.append(palabra)\n",
    "    \n",
    "freq_palabras = [fdist[palabra] for palabra in palabras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda2106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "log_freq_palabras = np.log(freq_palabras)\n",
    "log_ranking = np.log(rango)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=log_ranking, y=log_freq_palabras, ax=ax)\n",
    "ax.set_xlabel(\"Log(Rango)\")\n",
    "ax.set_ylabel(\"Log(Frecuencia)\")\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(log_ranking.reshape(-1, 1), log_freq_palabras.reshape(-1, 1))\n",
    "\n",
    "m = linreg.coef_[0][0]\n",
    "b = linreg.intercept_[0]\n",
    "print(f\"Pendiente: {m}, Interseccion: {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7373e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg = linreg.predict(log_ranking.reshape(-1,1))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=log_ranking, y=log_freq_palabras, ax=ax)\n",
    "ax.set_xlabel(\"Log(Rango)\")\n",
    "ax.set_ylabel(\"Log(Frecuencia)\")\n",
    "sns.lineplot(x=log_ranking, y=y_reg.flatten(), color='red', ax=ax)\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.exp(b)\n",
    "s = -m\n",
    "\n",
    "print(f\"Constante de normalizacion: {C}. Frecuencia de la palabra mas comun: {fdist[palabras[0]]}\")\n",
    "\n",
    "print(f\"Exponente de la ley de Zipf: {s}\")\n",
    "\n",
    "\n",
    "### Discutir por que."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de774d1",
   "metadata": {},
   "source": [
    "## 3. Palabras importantes por clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_texto(texto:str) -> str:\n",
    "    texto = texto.lower()\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    tokens_normalizados = [token for token in tokens if token not in spanish_stopwords]\n",
    "    return \" \".join(tokens_normalizados)\n",
    "\n",
    "data_normalizada = data.copy()\n",
    "data_normalizada[\"Review\"] = data_normalizada[\"Review\"].apply(normalizar_texto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palabras mas utilizadas por clase\n",
    "for clase in sorted(data_normalizada[\"Polarity\"].unique()):\n",
    "    corpus_r = data_normalizada.loc[data_normalizada[\"Polarity\"] == clase,\"Review\"].tolist()\n",
    "    \n",
    "    tokens_r = [word for review in corpus_r for word in tokenizer.tokenize(review)]\n",
    "    freq = FreqDist(tokens_r)\n",
    "\n",
    "    palabra_array = []\n",
    "    frecuencia_array = []\n",
    "    for palabra, frecuencia in freq.most_common(10):\n",
    "        palabra_array.append(palabra)\n",
    "        frecuencia_array.append(frecuencia)\n",
    "\n",
    "    sns.barplot(x=frecuencia_array, y=palabra_array, orient=\"h\")\n",
    "    plt.title(f\"Palabras más comunes en reseñas de clase {clase:.0f}\")\n",
    "    plt.xlabel(\"Frecuencia\")\n",
    "    plt.ylabel(\"Palabra\")\n",
    "    plt.savefig(f\"./figures/palabras_comunes_clase_{clase:.0f}.png\", bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e67491",
   "metadata": {},
   "source": [
    "## 4. Patrones gramaticales (POS tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#!python -m spacy download es_core_news_sm\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_pos_tagging(texto):\n",
    "    doc = nlp(texto)\n",
    "    return \" \".join([token.pos_ for token in doc])\n",
    "\n",
    "# Utilizamos el corpus sin preprocesar para mantener la gramatica.\n",
    "data[\"pos\"] = data[\"Review\"].apply(aplicar_pos_tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3bc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n=4):\n",
    "    \"\"\"\n",
    "    Toma una cadena de texto, la divide en tokens y devuelve una lista de n-gramas.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    if len(tokens) < n:\n",
    "        return []\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f9be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "data[\"tetragram\"] = data[\"pos\"].apply(lambda x: get_ngrams(x, n=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tetragram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc0943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los tetragramas más comunes para cada clase\n",
    "class_counts = {}\n",
    "for cls in sorted(data['Polarity'].unique()):\n",
    "    class_df = data[data['Polarity'] == cls]\n",
    "\n",
    "    all_tetragrams = list(chain.from_iterable(class_df['tetragram']))\n",
    "    counter = Counter(all_tetragrams)\n",
    "    class_counts[cls] = counter.most_common(5)\n",
    "    \n",
    "for cls, common_tetragrams in class_counts.items():\n",
    "    print(f\"\\nLos 5 tetragramas más comunes para la clase {cls:.0f} son:\")\n",
    "    for tetragram, count in common_tetragrams:\n",
    "        print(f\"- '{tetragram}' : {count} veces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b402d47",
   "metadata": {},
   "source": [
    "## 5. Representaciones BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb906e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tfidf_vectorizer.fit_transform(data[\"Review\"]).toarray()\n",
    "X_tf = count_vectorizer.fit_transform(data[\"Review\"]).toarray()\n",
    "y = data[\"Polarity\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ede617",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "tf_features = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2088434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "def get_chi2_n_mi(X, y):\n",
    "    chi2_scores, p_values = chi2(X, y)\n",
    "\n",
    "    return chi2_scores, p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_chi2, tf_p = get_chi2_n_mi(X_tf, y)\n",
    "tfidf_chi2, tfidf_p = get_chi2_n_mi(X_tfidf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Select the top 20 features using Chi-Squared\n",
    "k = 20\n",
    "\n",
    "tf_selector_chi2 = SelectKBest(chi2, k=k)\n",
    "X_new_chi2 = tf_selector_chi2.fit_transform(X_tf, y)\n",
    "\n",
    "tfidf_selector_chi2 = SelectKBest(chi2, k=k)\n",
    "X_new_tfidf_chi2 = tfidf_selector_chi2.fit_transform(X_tfidf, y)\n",
    "\n",
    "# Get the indices of the selected features\n",
    "tf_selected_features_indices = tf_selector_chi2.get_support(indices=True)\n",
    "tfidf_selected_features_indices = tfidf_selector_chi2.get_support(indices=True)\n",
    "\n",
    "# Map indices back to feature names\n",
    "top_features_tf_chi2 = [tf_features[i] for i in tf_selected_features_indices]\n",
    "top_features_tfidf_chi2 = [tfidf_features[i] for i in tfidf_selected_features_indices]\n",
    "\n",
    "print(f\"Top {k} features by Chi-Squared (TF): {top_features_tf_chi2}\")\n",
    "print(f\"Top {k} features by Chi-Squared (TF-IDF): {top_features_tfidf_chi2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "contador_tf = 0\n",
    "contador_tfidf = 0\n",
    "\n",
    "for i in range(20):\n",
    "    if top_features_tf_chi2[i] in spanish_stopwords:\n",
    "        contador_tf += 1\n",
    "    if top_features_tfidf_chi2[i] in spanish_stopwords:\n",
    "        contador_tfidf += 1\n",
    "\n",
    "print(f\"Stopwords TF: {contador_tf}\")\n",
    "print(f\"Stopwords TF-IDF: {contador_tfidf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42648692",
   "metadata": {},
   "source": [
    "## 6. Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6c2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
    "count_vectorizer = CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7cc263",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tfidf_vectorizer.fit_transform(data[\"Review\"]).toarray()\n",
    "X_tf = count_vectorizer.fit_transform(data[\"Review\"]).toarray()\n",
    "y = data[\"Polarity\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "tf_features = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc826d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_chi2, tf_p = get_chi2_n_mi(X_tf, y)\n",
    "tfidf_chi2, tfidf_p = get_chi2_n_mi(X_tfidf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Select the top 20 features using Chi-Squared\n",
    "k = 20\n",
    "\n",
    "tf_selector_chi2 = SelectKBest(chi2, k=k)\n",
    "X_new_chi2 = tf_selector_chi2.fit_transform(X_tf, y)\n",
    "\n",
    "tfidf_selector_chi2 = SelectKBest(chi2, k=k)\n",
    "X_new_tfidf_chi2 = tfidf_selector_chi2.fit_transform(X_tfidf, y)\n",
    "\n",
    "# Get the indices of the selected features\n",
    "tf_selected_features_indices = tf_selector_chi2.get_support(indices=True)\n",
    "tfidf_selected_features_indices = tfidf_selector_chi2.get_support(indices=True)\n",
    "\n",
    "# Map indices back to feature names\n",
    "top_features_tf_chi2 = [tf_features[i] for i in tf_selected_features_indices]\n",
    "top_features_tfidf_chi2 = [tfidf_features[i] for i in tfidf_selected_features_indices]\n",
    "\n",
    "print(f\"Top {k} features by Chi-Squared (TF): {top_features_tf_chi2}\")\n",
    "print(f\"Top {k} features by Chi-Squared (TF-IDF): {top_features_tfidf_chi2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "contador_tf = 0\n",
    "contador_tfidf = 0\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    if top_features_tf_chi2[i].split()[0] in spanish_stopwords:\n",
    "        contador_tf += 1\n",
    "    if top_features_tf_chi2[i].split()[1] in spanish_stopwords:\n",
    "        contador_tf += 1\n",
    "    if top_features_tfidf_chi2[i].split()[0] in spanish_stopwords:\n",
    "        contador_tfidf += 1\n",
    "    if top_features_tfidf_chi2[i].split()[1] in spanish_stopwords:\n",
    "        contador_tfidf += 1\n",
    "\n",
    "print(f\"Stopwords TF: {contador_tf}\")\n",
    "print(f\"Stopwords TF-IDF: {contador_tfidf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intr = set(top_features_tf_chi2).intersection(set(top_features_tfidf_chi2))\n",
    "len(intr)\n",
    "\n",
    "solo_tf = set(top_features_tf_chi2) - set(top_features_tfidf_chi2)\n",
    "solo_tfidf = set(top_features_tfidf_chi2) - set(top_features_tf_chi2)\n",
    "\n",
    "print(f\"Solo en TF: {solo_tf}\")\n",
    "print(f\"Solo en TF-IDF: {solo_tfidf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd71b69",
   "metadata": {},
   "source": [
    "## 7. Word2Vec y analogias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d2272",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(sentences=doc_tokens, vector_size=50, window=2, sg=1, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6650ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogies(model: Word2Vec, positive, negative):\n",
    "    \"\"\"\n",
    "    Obtiene las palabras más similares a una palabra dada, utilizando un modelo de Word2Vec.\n",
    "    \"\"\"\n",
    "    return model.wv.most_similar(positive=positive, negative=negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cccb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogies = get_analogies(w2v, positive=['habitacion','restaurante'], negative=['hotel'])\n",
    "print(analogies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e05da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogies = get_analogies(w2v, positive=['mesa','hotel'], negative=['restaurante'])\n",
    "print(analogies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50aa0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogies = get_analogies(w2v, positive=['deliciosa','restaurante'], negative=None)\n",
    "print(analogies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df695f54",
   "metadata": {},
   "source": [
    "## 8. Embeddings de documento y clusterizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0155ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(doc, w2v):\n",
    "    \"\"\"Takes tokenized document to vector representation.\"\"\"\n",
    "    word_vectors = np.array([w2v.wv[word] for word in doc if word in w2v.wv])\n",
    "    return word_vectors.mean(axis=0)\n",
    "\n",
    "doc_vectors = np.array([doc2vec(doc, w2v) for doc in doc_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073c51a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroides = kmeans.cluster_centers_\n",
    "centroides.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_centroides = np.zeros((5000,5))\n",
    "for i, doc_vec in enumerate(doc_vectors):\n",
    "    for j in range(5):\n",
    "        dist_centroides[i, j] = np.linalg.norm(doc_vec - centroides[j])\n",
    "\n",
    "doc_mas_cercanos = np.argmin(dist_centroides, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7665f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_mas_cercanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d601a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Documentos mas cercanos a cada centroide:\")\n",
    "for i, doc in enumerate(doc_mas_cercanos):\n",
    "    print(f\"\\n- Centroide {i}\\nPolaridad: {data.loc[doc, 'Polarity']}\\nDocumento:\\n{documentos[doc]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb55b8d",
   "metadata": {},
   "source": [
    "## Clasificacion con particion 70/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c81192",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[\"Polarity\"].astype(int).values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "def evaluar_bow(BOW_train, BOW_test, val_y, tr_y):\n",
    "    # Parámetro de complejidad del SVM, se proponen estos\n",
    "    # y se recorrerán con GridSearch\n",
    "    parameters = {\"C\": [.05, .12, .25, .5, 1, 2, 4]}\n",
    "    # Tratar de penalizar con base a la proporción de ejemplos\n",
    "    # en cada clase\n",
    "    svr = svm.LinearSVC(class_weight='balanced', max_iter=10000)\n",
    "    grid = GridSearchCV(estimator=svr, param_grid=parameters,\n",
    "                        n_jobs=6, scoring=\"f1_macro\", cv=5)\n",
    "    grid.fit(BOW_train, tr_y)\n",
    "    y_pred = grid.predict(BOW_test)\n",
    "\n",
    "    confusion = \"\\n\" + str(confusion_matrix(val_y, y_pred))\n",
    "    class_report = \"\\n\" + classification_report(val_y, y_pred, digits=3)\n",
    "\n",
    "    return {\n",
    "        \"confusion_matrix\": confusion,\n",
    "        \"classification_report\": class_report\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "x_train_index, x_test_index, y_train, y_test = train_test_split(\n",
    "    np.arange(len(doc_vectors)), labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8f4782",
   "metadata": {},
   "source": [
    "### a) Sin preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c95be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v vectors de tokens sin preprocesar = doc_vectors\n",
    "X_train = doc_vectors[x_train_index]\n",
    "X_test = doc_vectors[x_test_index]\n",
    "\n",
    "y_train = labels[x_train_index]\n",
    "y_test = labels[x_test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee31a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluar_bow(X_train, X_test, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a42d1d9",
   "metadata": {},
   "source": [
    "### b) Con minusculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos documentos en minusculas y sin stopwords\n",
    "documentos_minusculas = [doc.lower() for doc in documentos]\n",
    "documentos_minusculas_stp = [\" \".join([token for token in doc.split() if token not in spanish_stopwords]) for doc in documentos_minusculas]\n",
    "\n",
    "# Creamos tokenss\n",
    "doc_min_tokens = [tokenizer.tokenize(doc) for doc in documentos_minusculas_stp]\n",
    "\n",
    "# Creamos el modelo word2vec con el tokenizado/preprocesado anterior\n",
    "w2v_min_tokens = Word2Vec(sentences=doc_min_tokens, vector_size=50, window=2, sg=1, seed=1)\n",
    "\n",
    "doc_min_w2v = np.array([doc2vec(doc, w2v_min_tokens) for doc in doc_min_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = doc_min_w2v[x_train_index]\n",
    "X_test = doc_min_w2v[x_test_index]\n",
    "y_train = labels[x_train_index]\n",
    "y_test = labels[x_test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluar_bow(X_train, X_test, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42818a4",
   "metadata": {},
   "source": [
    "### c) Minusculas, stemming/lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51501e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spanish stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_min_stem_tokens = [[stemmer.stem(token) for token in doc] for doc in doc_min_tokens]\n",
    "\n",
    "w2v_min_stem = Word2Vec(sentences=doc_min_stem_tokens, vector_size=50, window=2, sg=1, seed=1)\n",
    "\n",
    "doc_min_stem_w2v = np.array([doc2vec(doc, w2v_min_stem) for doc in doc_min_stem_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = doc_min_stem_w2v[x_train_index]\n",
    "X_test = doc_min_stem_w2v[x_test_index]\n",
    "y_train = labels[x_train_index]\n",
    "y_test = labels[x_test_index]\n",
    "\n",
    "metrics = evaluar_bow(X_train, X_test, y_test, y_train)\n",
    "\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57285c",
   "metadata": {},
   "source": [
    "### d) minusculas, stemming, y filtrado de palabras con frecuencia minima de 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de32c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_min_stem = [token for doc_token in doc_min_stem_tokens for token in doc_token]\n",
    "fdist = FreqDist(corpus_min_stem)\n",
    "invalid_tokens = set([token for token, freq in fdist.items() if freq < 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_min_stem_freq_tokens = [[token for token in doc if token not in invalid_tokens] for doc in doc_min_stem_tokens]\n",
    "\n",
    "w2v_min_stem_freq = Word2Vec(sentences=doc_min_stem_freq_tokens, vector_size=50, window=2, sg=1, seed=1)\n",
    "\n",
    "doc_min_stem_freq_w2v = np.array([doc2vec(doc, w2v_min_stem_freq) for doc in doc_min_stem_freq_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274fc571",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = doc_min_stem_freq_w2v[x_train_index]\n",
    "X_test = doc_min_stem_freq_w2v[x_test_index]\n",
    "y_train = labels[x_train_index]\n",
    "y_test = labels[x_test_index]\n",
    "\n",
    "metrics = evaluar_bow(X_train, X_test, y_test, y_train)\n",
    "\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5b827",
   "metadata": {},
   "source": [
    "## 10. LSA con 50 topicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d6807",
   "metadata": {},
   "source": [
    "### Building the matrix W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d84f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Preprocesamiento y vocabulario\n",
    "# -----------------------------\n",
    "def construir_vocab(corpus_tokens, min_count=1):\n",
    "    \"\"\"\n",
    "    Construye vocabulario a partir de una lista de listas de tokens.\n",
    "    min_count: frecuencia mínima para incluir palabra en el vocabulario.\n",
    "    \"\"\"\n",
    "    freqs = Counter()\n",
    "    for tokens in corpus_tokens:\n",
    "        freqs.update(tokens)\n",
    "    vocab = [w for w, c in freqs.items() if c >= min_count]\n",
    "    vocab.sort()  # orden estable\n",
    "    word2id = {w: i for i, w in enumerate(vocab)}\n",
    "    return vocab, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) Matriz de coocurrencia W\n",
    "# -----------------------------\n",
    "def matriz_coocurrencia(corpus, tokenizer = RegexpTokenizer(r\"\\b\\w+\\b\"), window_size=2, min_count=1, directed=True):\n",
    "    \"\"\"\n",
    "    Construye la matriz W (palabra objetivo x palabra contexto).\n",
    "    - window_size: tamaño de la ventana simétrica alrededor del target.\n",
    "    - directed=True: solo cuenta (i -> j). Si False, también suma (j -> i).\n",
    "    Devuelve: (W_csr, vocab, word2id)\n",
    "    \"\"\"\n",
    "    # Tokenizar corpus\n",
    "    corpus_tokens = [tokenizer.tokenize(doc) for doc in corpus]\n",
    "    # Vocabulario\n",
    "    vocab, word2id = construir_vocab(corpus_tokens, min_count=min_count)\n",
    "    V = len(vocab)\n",
    "\n",
    "    rows, cols, data = [], [], []\n",
    "\n",
    "    for tokens in corpus_tokens:\n",
    "        ids = [word2id[w] for w in tokens if w in word2id]\n",
    "        n = len(ids)\n",
    "        for i, wi in enumerate(ids):\n",
    "            # ventana [i-window_size, i+window_size], excluyendo i\n",
    "            left = max(0, i - window_size)\n",
    "            right = min(n, i + window_size + 1)\n",
    "            for j in range(left, right):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                wj = ids[j]\n",
    "                # cuenta ocurrencia en el contexto\n",
    "                rows.append(wi)\n",
    "                cols.append(wj)\n",
    "                data.append(1.0)\n",
    "                # si quieres matriz estrictamente simétrica, puedes sumar también el espejo\n",
    "                if not directed:\n",
    "                    rows.append(wj)\n",
    "                    cols.append(wi)\n",
    "                    data.append(1.0)\n",
    "\n",
    "    if not rows:\n",
    "        # corpus vacío o vocabulario filtrado demasiado estricto\n",
    "        W = csr_matrix((V, V), dtype=np.float64)\n",
    "    else:\n",
    "        W = coo_matrix((np.array(data, dtype=np.float64),\n",
    "                        (np.array(rows), np.array(cols))), shape=(V, V)).tocsr()\n",
    "\n",
    "    return W, vocab, word2id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3552c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) LSA vía SVD truncado\n",
    "# -----------------------------\n",
    "def lsa_embeddings(W, k=10, random_state=0):\n",
    "    \"\"\"\n",
    "    Aplica SVD truncado a W:\n",
    "        W ≈ U_k Σ_k V_k^T\n",
    "    TruncatedSVD.fit_transform(W) ≈ U_k Σ_k  (embeddings de filas/palabras objetivo)\n",
    "    Devuelve: X (V x k) con embeddings densos.\n",
    "    \"\"\"\n",
    "    k = min(k, min(W.shape) - 1)  # seguridad\n",
    "    svd = TruncatedSVD(n_components=k, random_state=random_state)\n",
    "    X = svd.fit_transform(W)  # U_k Σ_k\n",
    "    return X, svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24414b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Utilidades: similitud de palabras\n",
    "# -----------------------------\n",
    "def similitud_palabras(palabra, vocab, X, topk=10):\n",
    "    \"\"\"\n",
    "    Devuelve top-k palabras más similares (coseno) a 'palabra'\n",
    "    usando los embeddings X.\n",
    "    \"\"\"\n",
    "    if palabra not in vocab:\n",
    "        raise ValueError(f\"'{palabra}' no está en el vocabulario.\")\n",
    "    idx = vocab.index(palabra)\n",
    "    v = X[idx:idx+1]  # (1, k)\n",
    "    sims = cosine_similarity(v, X).flatten()  # (V,)\n",
    "    # ordenar por similitud decreciente; ignorar índice de la propia palabra\n",
    "    orden = np.argsort(-sims)\n",
    "    vecinos = [(vocab[i], float(sims[i])) for i in orden if i != idx][:topk]\n",
    "    return vecinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documentos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
