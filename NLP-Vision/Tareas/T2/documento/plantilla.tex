% !TeX recipe = Rtex
% Optimizado para compilación rápida
\documentclass[paper=letter, fontsize=11pt, draft=false]{scrartcl}

% Modifications to layout
\usepackage[shortlabels]{enumitem} % Incisos
\def\code#1{\texttt{#1}} % \code{} for monospaced text
\newcommand{\RNum}[1]{\footnotesize\uppercase\expandafter{\romannumeral #1\relax\normalsize}} % Roman numbers

\usepackage{subcaption} % 2x2 graphs
\usepackage{mwe}
\usepackage{float} % [H] in graphics
\usepackage[hidelinks]{hyperref}  % Hipervínculos en la TOC

\usepackage{booktabs,siunitx,listings}
\usepackage[most]{tcolorbox}
\tcbuselibrary{theorems}
\usepackage{cleveref}

% Typography and layout packages
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage[spanish,es-nodecimaldot]{babel} % Language and hyphenation
\usepackage{amsmath, amsfonts, amsthm, amssymb} % Math packages
\newtheorem{definition}{Definición} % definition
\usepackage{fancyvrb}
\usepackage{sectsty} % Customize section commands
\usepackage{geometry} % Modify margins
\usepackage{titlesec} % Customize section titles
\geometry{margin=3cm,top=2.5cm,bottom=2.5cm} % Simplified geometry
\allsectionsfont{\centering \normalfont\scshape} % Center and style section titles

% Header and footer customization
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead[L]{\slshape} % Remove section title from header
\fancyhead[C]{} % Header center
\fancyhead[R]{\thepage} % Header right with page number
\fancyfoot[L]{} % Footer left
\fancyfoot[C]{} % Footer center
\fancyfoot[R]{\small \slshape Gustavo Hernández Angeles} % Footer right
\renewcommand{\headrulewidth}{0.4pt} % Header rule
\renewcommand{\footrulewidth}{0.4pt} % Footer rule
\setlength{\headheight}{14.5pt} % Header height

% Paragraph settings
\setlength\parindent{0pt}
\setlength{\parskip}{1ex}

% Section spacing
\titlespacing*{\section}{0cm}{0.50cm}{0.25cm}

% --- Theorems, lemma, corollary, postulate, definition ---
\definecolor{Pantone209C}{HTML}{64293e}

\newcounter{problemcounter}

\numberwithin{equation}{section} % Number equations within problems
\numberwithin{figure}{section} % Number figures within problems
\numberwithin{table}{section} % Number tables within problems
\numberwithin{subsection}{section} 

\newtcbtheorem[auto counter]{problem}{Ejercicio}{
    enhanced,
    breakable,
    colback = gray!5,
    colframe = gray!5,
    boxrule = 0.5pt,
    sharp corners,
    borderline west = {2mm}{0mm}{Pantone209C},
    fonttitle = \bfseries\sffamily,
    coltitle = Pantone209C,
    drop fuzzy shadow,
    parbox = false,
    before skip = 3ex,
    after skip = 3ex
}{problem}
\makeatletter
\renewenvironment{problem}[2][]{%
    \refstepcounter{problemcounter}%
    \addcontentsline{toc}{section}{\protect\numberline{\theproblemcounter}Ejercicio \theproblemcounter: #2}%
    \begin{tcolorbox}[
        enhanced,
        breakable,
        colback = gray!5,
        colframe = gray!5,
        boxrule = 0.5pt,
        sharp corners,
        borderline west = {2mm}{0mm}{Pantone209C},
        fonttitle = \bfseries\sffamily,
        coltitle = Pantone209C,
        drop fuzzy shadow,
        parbox = false,
        before skip = 3ex,
        after skip = 3ex,
        title={Ejercicio \theproblemcounter: #2}
    ]
}{%
    \end{tcolorbox}
}
\makeatother

\tcbuselibrary{skins, breakable}
% Custom command for a horizontal rule
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 

% Custom section titles with numbering
\titleformat{\section}
{\normalfont\Large\bfseries}{}{1em}{}

\titleformat{\subsection}
{\normalfont\large\bfseries}{}{1em}{}

\titleformat{\subsubsection}
{\normalfont\normalsize\bfseries}{}{1em}{}

% Title and author
\title{	
    \begin{center}
        \includegraphics[width=3cm]{figure/template/cimat-logo.png} % Adjust size as needed
    \end{center}
    \vspace{0.5cm}
    \normalfont \normalsize 
    \textbf{\Large   Centro de Investigación en Matemáticas} \\
    \Large Unidad Monterrey \\ [25pt] 
    \horrule{1pt} \\[0.4cm] % Thin top horizontal rule
    \huge Análisis de Texto e Imágenes\\
    \Large Generación y Clasificación de Texto con Deep Learning\\ 
    \horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{\large Gustavo Hernández Angeles}    

\date{\normalsize\today} % Today's date

\begin{document}
\maketitle % Print the title
\thispagestyle{empty}
\newpage

\tableofcontents
\newpage



%%%%%%%%%%% PROBLEMA 1 %%%%%%%%%%%
\section{Parte A: Generación de Texto}

\subsection{Datos}

En esta sección se explica el proceso de recopilación y limpieza de datos para entrenar los modelos de generación de texto utilizando letras de canciones.

\subsubsection{Recopilación de datos}

Se utilizaron dos scripts de Python para scrapear las letras de las canciones de un artista específico desde la plataforma \textit{Genius}. El primer script (\texttt{scrape\_songs\_alpaca.py}) realiza las siguientes tareas:

\begin{itemize}
    \item Autenticación en la API de Genius.
    \item Búsqueda de canciones (URLs) del artista.
    \item Extracción de las letras de las canciones.
    \item Limpieza y preprocesamiento de las letras en formato Alpaca para Fine-Tuning.
    \item Almacenamiento de las letras en un archivo de texto.
\end{itemize}

El segundo script (\texttt{preprocess\_scraped\_songs.py}) se encarga de procesar la salida del primer script para generar un archivo .txt con el formato adecuado para el entrenamiento de los modelos RNN/LSTM/GRU.

La selección de artistas y el número de canciones a scrapear se encuentran configuradas directamente en el código del primer script. En este caso, elegí a \textit{Kendrick Lamar}, \textit{Kanye West} y \textit{Jay-Z} debido al género compartido \textit{Hip-Hop/Rap} y la riqueza lírica de sus canciones. El número de canciones por artista se estableció en 100, lo que da un total aproximado de 300 canciones.

La ejecución de ambos scripts se realiza desde la terminal con la serie de comandos:

\begin{center}
\texttt{python ./codigo/generative/scrape\_songs\_alpaca.py} \\
\texttt{python ./codigo/generative/preprocess\_scraped\_songs.py}
\end{center}

\textbf{Nota:} Es importante asegurarse de tener la clave de API de Genius configurada en un archivo \texttt{.env} en el mismo directorio que el script. Ejemplo: \texttt{GENIUS\_API\_TOKEN="tu\_token\_aqui"}.

Al finalizar la ejecución, obtenemos distintos archivos para ambos formatos de salida particionados en conjuntos de entrenamiento y prueba. Además, también se generan los diccionarios necesarios para el preprocesamiento de texto en los modelos RNN/LSTM/GRU, y estadísticas del conjunto de datos. Los archivos generados son almacenados en el directorio \texttt{./data/text\_gen/} y son los siguientes:
\begin{itemize}
    \item \texttt{train\_lyrics\_alpaca.json} y
    \item  \texttt{test\_lyrics\_alpaca.json} (formato Alpaca).
    \item \texttt{train\_lyrics.txt} y
    \item \texttt{test\_lyrics.txt} (formato para RNN/LSTM/GRU).
    \item \texttt{vocab\_char.json} y
    \item \texttt{vocab\_word.json} (diccionarios de caracteres y palabras).
    \item \texttt{dataset\_info.json} (estadísticas del conjunto de datos).
\end{itemize}



\subsubsection{Limpieza y preprocesamiento del texto}

Genius provee las letras en formato HTML dentro de \texttt{lyrics containers}, los cuales son etiquetas con el atributo \texttt{data-lyrics-container="true"}. El script extrae el texto de estas etiquetas y realiza las siguientes operaciones de limpieza:

\begin{itemize}
    \item Eliminación de etiquetas HTML.
    \item Eliminación de líneas en blanco y espacios innecesarios.
    \item Crea cada registro en formato Alpaca para el fine-tuning.
\end{itemize}


He decidido no eliminar las anotaciones de las canciones (como [Coro], [Verso 1], etc.) ya que pueden proporcionar contexto adicional al modelo durante el entrenamiento, además de que pueden ser útiles para la generación de texto (haciendo que el modelo también genere anotaciones). Además, se conservaron los metadatos como nombre de la canción y artista, ya que pueden ser útiles para futuras referencias o análisis.

Al realizar Fine-Tuning sobre el LLM, es conveniente proporcionar contexto adicional (y acorde) al modelo. Debido a que en este caso nos decidimos por utilizar un modelo instructivo, el formato Alpaca es adecuado para este propósito. Este formato contiene campos para la instrucción, entrada y salida, lo que permite al modelo aprender a generar texto basado en instrucciones específicas de un solo turno.
\begin{verbatim}
{
    "instruction": "<INSTRUCCIÓN>",
    "input": "<ENTRADA>",
    "output": "<SALIDA>"
}
\end{verbatim}

donde:
\begin{itemize} 
    \item \texttt{<INSTRUCCIÓN>} es una cadena que indica la tarea a realizar, en este caso: ``You are a hip-hop artist, helping people write lyrics.'' o variantes.
    \item \texttt{<ENTRADA>} es una cadena que proporciona contexto adicional, en este caso: ``Help me write a song in the style of \texttt{<ARTISTA>}.''
    \item \texttt{<SALIDA>} es la letra de la canción.
\end{itemize}

Por otro lado, al utilizar modelos RNN/LSTM/GRU basta con simplemente tener las letras en texto plano, ya que estos modelos no requieren el mismo nivel de contexto que los LLMs. Sin embargo, es importante asegurarse de que el texto esté limpio y bien formateado para evitar problemas durante el entrenamiento. El script de preprocesamiento convierte los textos de formato Alpaca a texto plano, asegurándose de que cada letra esté separada por los delimitadores de cada canción (En este caso \texttt{<|song\_start|>} y \texttt{<song\_end>}).



\subsection{Modelos y Entrenamiento}

En esta sección se describen los modelos utilizados para la generación de texto, así como los detalles del proceso de entrenamiento.

\subsubsection{Transformers: Mistral 7B Instruct}






\end{document}
