% !TeX recipe = Rtex
% Optimizado para compilación rápida
\documentclass[paper=letter, fontsize=11pt, draft=false]{scrartcl}

% Modifications to layout
\usepackage[shortlabels]{enumitem} % Incisos
\def\code#1{\texttt{#1}} % \code{} for monospaced text
\newcommand{\RNum}[1]{\footnotesize\uppercase\expandafter{\romannumeral #1\relax\normalsize}} % Roman numbers

\usepackage{subcaption} % 2x2 graphs
\usepackage{mwe}
\usepackage{float} % [H] in graphics
\usepackage[hidelinks]{hyperref}  % Hipervínculos en la TOC

\usepackage{booktabs,siunitx,listings}
\usepackage[most]{tcolorbox}
\tcbuselibrary{theorems}
\usepackage{cleveref}

% Typography and layout packages
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage[spanish,es-nodecimaldot]{babel} % Language and hyphenation
\usepackage{amsmath, amsfonts, amsthm, amssymb} % Math packages
\newtheorem{definition}{Definición} % definition
\usepackage{fancyvrb}
\usepackage{sectsty} % Customize section commands
\usepackage{geometry} % Modify margins
\usepackage{titlesec} % Customize section titles
\geometry{margin=3cm,top=2.5cm,bottom=2.5cm} % Simplified geometry
\allsectionsfont{\centering \normalfont\scshape} % Center and style section titles

% Header and footer customization
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead[L]{\slshape} % Remove section title from header
\fancyhead[C]{} % Header center
\fancyhead[R]{\thepage} % Header right with page number
\fancyfoot[L]{} % Footer left
\fancyfoot[C]{} % Footer center
\fancyfoot[R]{\small \slshape Gustavo Hernández Angeles} % Footer right
\renewcommand{\headrulewidth}{0.4pt} % Header rule
\renewcommand{\footrulewidth}{0.4pt} % Footer rule
\setlength{\headheight}{14.5pt} % Header height

% Paragraph settings
\setlength\parindent{0pt}
\setlength{\parskip}{1ex}

% Section spacing
\titlespacing*{\section}{0cm}{0.50cm}{0.25cm}

% --- Theorems, lemma, corollary, postulate, definition ---
\definecolor{Pantone209C}{HTML}{64293e}

\newcounter{problemcounter}

\numberwithin{equation}{section} % Number equations within problems
\numberwithin{figure}{section} % Number figures within problems
\numberwithin{table}{section} % Number tables within problems
\numberwithin{subsection}{section} 

\newtcbtheorem[auto counter]{problem}{Ejercicio}{
    enhanced,
    breakable,
    colback = gray!5,
    colframe = gray!5,
    boxrule = 0.5pt,
    sharp corners,
    borderline west = {2mm}{0mm}{Pantone209C},
    fonttitle = \bfseries\sffamily,
    coltitle = Pantone209C,
    drop fuzzy shadow,
    parbox = false,
    before skip = 3ex,
    after skip = 3ex
}{problem}
\makeatletter
\renewenvironment{problem}[2][]{%
    \refstepcounter{problemcounter}%
    \addcontentsline{toc}{section}{\protect\numberline{\theproblemcounter}Ejercicio \theproblemcounter: #2}%
    \begin{tcolorbox}[
        enhanced,
        breakable,
        colback = gray!5,
        colframe = gray!5,
        boxrule = 0.5pt,
        sharp corners,
        borderline west = {2mm}{0mm}{Pantone209C},
        fonttitle = \bfseries\sffamily,
        coltitle = Pantone209C,
        drop fuzzy shadow,
        parbox = false,
        before skip = 3ex,
        after skip = 3ex,
        title={Ejercicio \theproblemcounter: #2}
    ]
}{%
    \end{tcolorbox}
}
\makeatother

\tcbuselibrary{skins, breakable}
% Custom command for a horizontal rule
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 

% Custom section titles with numbering
\titleformat{\section}
{\normalfont\Large\bfseries}{}{1em}{}

\titleformat{\subsection}
{\normalfont\large\bfseries}{}{1em}{}

\titleformat{\subsubsection}
{\normalfont\normalsize\bfseries}{}{1em}{}

% Title and author
\title{	
    \begin{center}
        \includegraphics[width=3cm]{figure/template/cimat-logo.png} % Adjust size as needed
    \end{center}
    \vspace{0.5cm}
    \normalfont \normalsize 
    \textbf{\Large   Centro de Investigación en Matemáticas} \\
    \Large Unidad Monterrey \\ [25pt] 
    \horrule{1pt} \\[0.4cm] % Thin top horizontal rule
    \huge Análisis de Texto e Imágenes\\
    \Large Generación y Clasificación de Texto con Deep Learning\\ 
    \horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{\large Gustavo Hernández Angeles}    

\date{\normalsize\today} % Today's date

\begin{document}
\maketitle % Print the title
\thispagestyle{empty}
\newpage

\tableofcontents
\newpage



%%%%%%%%%%% PROBLEMA 1 %%%%%%%%%%%
\section{Parte A: Generación de Texto}

\subsection{Datos}

En esta sección se explica el proceso de recopilación y limpieza de datos para entrenar los modelos de generación de texto utilizando letras de canciones.

\subsubsection{Recopilación de datos}

Se utilizaron dos scripts de Python para scrapear las letras de las canciones de un artista específico desde la plataforma \textit{Genius}. El primer script (\texttt{scrape\_songs\_alpaca.py}) realiza las siguientes tareas:

\begin{itemize}
    \item Autenticación en la API de Genius.
    \item Búsqueda de canciones (URLs) del artista.
    \item Extracción de las letras de las canciones.
    \item Limpieza y preprocesamiento de las letras en formato Alpaca para Fine-Tuning.
    \item Almacenamiento de las letras en un archivo de texto.
\end{itemize}

El segundo script (\texttt{preprocess\_scraped\_songs.py}) se encarga de procesar la salida del primer script para generar un archivo .txt con el formato adecuado para el entrenamiento de los modelos RNN/LSTM/GRU.

La selección de artistas y el número de canciones a scrapear se encuentran configuradas directamente en el código del primer script. En este caso, elegí a \textit{Kendrick Lamar}, \textit{Kanye West} y \textit{Jay-Z} debido al género compartido \textit{Hip-Hop/Rap} y la riqueza lírica de sus canciones. El número de canciones por artista se estableció en 100, lo que da un total aproximado de 300 canciones.

La ejecución de ambos scripts se realiza desde la terminal con la serie de comandos:

\begin{center}
\texttt{python ./codigo/generative/scrape\_songs\_alpaca.py} \\
\texttt{python ./codigo/generative/preprocess\_scraped\_songs.py}
\end{center}

\textbf{Nota:} Es importante asegurarse de tener la clave de API de Genius configurada en un archivo \texttt{.env} en el mismo directorio que el script. Ejemplo: \texttt{GENIUS\_API\_TOKEN="tu\_token\_aqui"}.

Al finalizar la ejecución, obtenemos distintos archivos para ambos formatos de salida particionados en conjuntos de entrenamiento y prueba. Además, también se generan los diccionarios necesarios para el preprocesamiento de texto en los modelos RNN/LSTM/GRU, y estadísticas del conjunto de datos. Los archivos generados son almacenados en el directorio \texttt{./data/text\_gen/} y son los siguientes:
\begin{itemize}
    \item \texttt{train\_lyrics\_alpaca.json} y
    \item  \texttt{test\_lyrics\_alpaca.json} (formato Alpaca).
    \item \texttt{train\_lyrics.txt} y
    \item \texttt{test\_lyrics.txt} (formato para RNN/LSTM/GRU).
    \item \texttt{vocab\_char.json} y
    \item \texttt{vocab\_word.json} (diccionarios de caracteres y palabras).
    \item \texttt{dataset\_info.json} (estadísticas del conjunto de datos).
\end{itemize}



\subsubsection{Limpieza y preprocesamiento del texto}

Genius provee las letras en formato HTML dentro de \texttt{lyrics containers}, los cuales son etiquetas con el atributo \texttt{data-lyrics-container="true"}. El script extrae el texto de estas etiquetas y realiza las siguientes operaciones de limpieza:

\begin{itemize}
    \item Eliminación de etiquetas HTML.
    \item Eliminación de líneas en blanco y espacios innecesarios.
    \item Crea cada registro en formato Alpaca para el fine-tuning.
\end{itemize}


He decidido no eliminar las anotaciones de las canciones (como [Coro], [Verso 1], etc.) ya que pueden proporcionar contexto adicional al modelo durante el entrenamiento, además de que pueden ser útiles para la generación de texto (haciendo que el modelo también genere anotaciones). Además, se conservaron los metadatos como nombre de la canción y artista, ya que pueden ser útiles para futuras referencias o análisis.

Al realizar Fine-Tuning sobre el LLM, es conveniente proporcionar contexto adicional (y acorde) al modelo. Debido a que en este caso nos decidimos por utilizar un modelo instructivo, el formato Alpaca es adecuado para este propósito. Este formato contiene campos para la instrucción, entrada y salida, lo que permite al modelo aprender a generar texto basado en instrucciones específicas de un solo turno.
\begin{verbatim}
{
    "instruction": "<INSTRUCCIÓN>",
    "input": "<ENTRADA>",
    "output": "<SALIDA>"
}
\end{verbatim}

donde:
\begin{itemize} 
    \item \texttt{<INSTRUCCIÓN>} es una cadena que indica la tarea a realizar, en este caso: ``You are a hip-hop artist, helping people write lyrics.'' o variantes.
    \item \texttt{<ENTRADA>} es una cadena que proporciona contexto adicional, en este caso: ``Help me write a song in the style of \texttt{<ARTISTA>}.''
    \item \texttt{<SALIDA>} es la letra de la canción.
\end{itemize}

Por otro lado, al utilizar modelos RNN/LSTM/GRU basta con simplemente tener las letras en texto plano, ya que estos modelos no requieren el mismo nivel de contexto que los LLMs. Sin embargo, es importante asegurarse de que el texto esté limpio y bien formateado para evitar problemas durante el entrenamiento. El script de preprocesamiento convierte los textos de formato Alpaca a texto plano, asegurándose de que cada letra esté separada por los delimitadores de cada canción (En este caso \texttt{<|song\_start|>} y \texttt{<song\_end>}).


\subsection{Transformers}

En este trabajo se utilizó el modelo \texttt{unsloth/mistral-7B-instruct-v0.3-bnb-4bit} como base para el fine-tuning. Este modelo es una variante del modelo creado por Mistral: Mistral-7B \cite{jiang2023mistral7b}, y optimizado por \textit{Unsloth} para tareas de instrucción y cuantizado a 4 bits para reducir el tamaño del modelo.

El framework utilizado para el fine-tuning fue \texttt{transformers} de Hugging Face, junto con \texttt{peft} para la implementación de Low-Rank Adaptation (LoRA). La elección de LoRA se debe a su eficiencia en términos de memoria y tiempo de entrenamiento, permitiendo adaptar grandes modelos preentrenados con un costo computacional reducido \cite{hu2022lora}.

\textbf{Tokenizador y representación.} El modelo base emplea un tokenizador sub‐palabras basado en SentencePiece con un vocabulario de aproximadamente 32K tokens \cite{kudo2018sentencepiece}. Este enfoque permite manejar adecuadamente la variabilidad léxica propia de letras de rap (slang, contracciones y variaciones estilísticas) sin inflar excesivamente el vocabulario. No se realizó entrenamiento de un tokenizador nuevo, ya que el objetivo fue un ajuste fino instructivo y no la adaptación desde cero a un dominio léxico extremadamente distinto.

\textbf{Formato de datos.} Cada ejemplo del conjunto de entrenamiento se construyó en formato tipo \textit{Alpaca}: \texttt{(instruction, input, output)} y posteriormente convertido a un \emph{chat template} estilo \texttt{alpaca} mediante \texttt{tokenizer.apply\_chat\_template}. Para cada canción se genera típicamente un par instrucción–respuesta, donde la instrucción solicita la generación de letras en el estilo de un artista específico y la salida contiene la letra correspondiente. Tras el mapeo, el campo final consumido por el entrenador es un único texto ya formateado.

\textbf{Tamaño del conjunto.} El corpus curado contiene 239 ejemplos de entrenamiento y 60 de prueba (correspondientes a canciones únicas). Con un tamaño de lote efectivo de 16 secuencias (lote por dispositivo = 2, acumulación de gradientes = 8), el número de pasos por época es:
\[
\text{steps/epoch} = \lceil 239 / 16 \rceil = 15.
\]
Se entrenaron 3 épocas, resultando en 45 pasos totales (se almacenaron checkpoints intermedios consistentes con este valor). Esto refleja un escenario de ajuste fino ligero, más orientado a estilo que a aprendizaje exhaustivo.

\textbf{Longitud de secuencia.} Se fijó \texttt{max\_seq\_length = 2048}. Dado que las letras individuales rara vez exceden este límite, se deshabilitó el \emph{packing} (\texttt{packing=False}) para evitar mezclar canciones dentro de la misma secuencia y preservar coherencia estructural (secciones como [Verse], [Chorus], etc.).

\textbf{Configuración LoRA.} Se aplicó LoRA con \texttt{r = 32}, \texttt{lora\_alpha = 64}, \texttt{lora\_dropout = 0.0} sobre los módulos de proyección de atención y MLP: \texttt{q\_proj, k\_proj, v\_proj, o\_proj, gate\_proj, up\_proj, down\_proj}. Suponiendo dimensiones del modelo base (d\_model = 4096, tamaño intermedio = 14336, 32 capas), el número aproximado de parámetros entrenables añadidos por LoRA es:
\[
\text{por capa} = 4 \times r (4096 + 4096) + 3 \times r (4096 + 14336) = 4(32 \cdot 8192) + 3(32 \cdot 18432) \approx 2{,}818{,}048,
\]
\[
\text{total} \approx 32 \times 2{,}818{,}048 \approx 90.2\text{M},
\]
lo que representa ~1.2\% de los =7.3B parámetros totales, pero concentra toda la capacidad de adaptación. Estos pesos LoRA se mantienen en precisión media (FP16/BF16) mientras los pesos base permanecen cuantizados a 4 bits \cite{dettmers2023qloraefficientfinetuningquantized}.

\textbf{Hiperparámetros de entrenamiento.} Se utilizaron: tasa de aprendizaje \texttt{2e-4} (scheduler lineal con calentamiento de 5 pasos), \texttt{epochs = 3}, \texttt{per\_device\_train\_batch\_size = 2}, \texttt{gradient\_accumulation\_steps = 8}, \texttt{weight\_decay = 0.01}, optimizador \texttt{AdamW 8-bit} \cite{dettmers2022optimizers}, y semilla \texttt{3407}. El tamaño de lote efectivo resultante es 16 ejemplos. No se aplicó regularización adicional (\texttt{lora\_dropout = 0}).

\textbf{Gestión de memoria y hardware.} El uso combinado de cuantización 4-bit + LoRA reduce el pico de VRAM necesario a un rango que típicamente cabe en una sola GPU de 12–16 GB manteniendo la ventana de contexto de 2048. El script registra memoria reservada y máxima por dispositivo antes y después del entrenamiento.

\subsubsection{Evaluación de la Generación de Texto}



\subsection{Modelos RNN/LSTM/GRU}



\newpage
\bibliographystyle{unsrt} % Elige un estilo (otros: abbrvnat, unsrtnat, etc.)
\bibliography{bib} % Indica el nombre de tu archivo .bib (sin la extensión)


\end{document}
