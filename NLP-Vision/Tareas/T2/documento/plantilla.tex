% !TeX recipe = Rtex
% Optimizado para compilación rápida
\documentclass[paper=letter, fontsize=11pt, draft=false]{scrartcl}

% Modifications to layout
\usepackage[shortlabels]{enumitem} % Incisos
\def\code#1{\texttt{#1}} % \code{} for monospaced text
\newcommand{\RNum}[1]{\footnotesize\uppercase\expandafter{\romannumeral #1\relax\normalsize}} % Roman numbers

\usepackage{subcaption} % 2x2 graphs
\usepackage{mwe}
\usepackage{float} % [H] in graphics
\usepackage[hidelinks]{hyperref}  % Hipervínculos en la TOC

\usepackage{booktabs,siunitx,listings}
\usepackage[most]{tcolorbox}
\tcbuselibrary{theorems}
\usepackage{cleveref}

% Typography and layout packages
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage[spanish,es-nodecimaldot]{babel} % Language and hyphenation
\usepackage{amsmath, amsfonts, amsthm, amssymb} % Math packages
\newtheorem{definition}{Definición} % definition
\usepackage{fancyvrb}
\usepackage{sectsty} % Customize section commands
\usepackage{geometry} % Modify margins
\usepackage{titlesec} % Customize section titles
\geometry{margin=3cm,top=2.5cm,bottom=2.5cm} % Simplified geometry
\allsectionsfont{\centering \normalfont\scshape} % Center and style section titles

% Header and footer customization
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead[L]{\slshape} % Remove section title from header
\fancyhead[C]{} % Header center
\fancyhead[R]{\thepage} % Header right with page number
\fancyfoot[L]{} % Footer left
\fancyfoot[C]{} % Footer center
\fancyfoot[R]{\small \slshape Gustavo Hernández Angeles} % Footer right
\renewcommand{\headrulewidth}{0.4pt} % Header rule
\renewcommand{\footrulewidth}{0.4pt} % Footer rule
\setlength{\headheight}{14.5pt} % Header height

% Paragraph settings
\setlength\parindent{0pt}
\setlength{\parskip}{1ex}

% Section spacing
\titlespacing*{\section}{0cm}{0.50cm}{0.25cm}

% --- Theorems, lemma, corollary, postulate, definition ---
\definecolor{Pantone209C}{HTML}{64293e}

\newcounter{problemcounter}

\numberwithin{equation}{section} % Number equations within problems
\numberwithin{figure}{section} % Number figures within problems
\numberwithin{table}{section} % Number tables within problems
\numberwithin{subsection}{section} 

\newtcbtheorem[auto counter]{problem}{Ejercicio}{
    enhanced,
    breakable,
    colback = gray!5,
    colframe = gray!5,
    boxrule = 0.5pt,
    sharp corners,
    borderline west = {2mm}{0mm}{Pantone209C},
    fonttitle = \bfseries\sffamily,
    coltitle = Pantone209C,
    drop fuzzy shadow,
    parbox = false,
    before skip = 3ex,
    after skip = 3ex
}{problem}
\makeatletter
\renewenvironment{problem}[2][]{%
    \refstepcounter{problemcounter}%
    \addcontentsline{toc}{section}{\protect\numberline{\theproblemcounter}Ejercicio \theproblemcounter: #2}%
    \begin{tcolorbox}[
        enhanced,
        breakable,
        colback = gray!5,
        colframe = gray!5,
        boxrule = 0.5pt,
        sharp corners,
        borderline west = {2mm}{0mm}{Pantone209C},
        fonttitle = \bfseries\sffamily,
        coltitle = Pantone209C,
        drop fuzzy shadow,
        parbox = false,
        before skip = 3ex,
        after skip = 3ex,
        title={Ejercicio \theproblemcounter: #2}
    ]
}{%
    \end{tcolorbox}
}
\makeatother

\tcbuselibrary{skins, breakable}
% Custom command for a horizontal rule
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 

% Custom section titles with numbering
\titleformat{\section}
{\normalfont\Large\bfseries}{\thesection}{1em}{}

\titleformat{\subsection}
{\normalfont\large\bfseries}{\thesubsection}{1em}{}

\titleformat{\subsubsection}
{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

% Title and author
\title{	
    \begin{center}
        \includegraphics[width=3cm]{figure/template/cimat-logo.png} % Adjust size as needed
    \end{center}
    \vspace{0.5cm}
    \normalfont \normalsize 
    \textbf{\Large   Centro de Investigación en Matemáticas} \\
    \Large Unidad Monterrey \\ [25pt] 
    \horrule{1pt} \\[0.4cm] % Thin top horizontal rule
    \huge Análisis de Texto e Imágenes\\
    \Large Generación y Clasificación de Texto con Deep Learning\\ 
    \horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{\large Gustavo Hernández Angeles}    

\date{\normalsize\today} % Today's date

\begin{document}
\maketitle % Print the title
\thispagestyle{empty}
\newpage

\tableofcontents
\newpage



%%%%%%%%%%% PROBLEMA 1 %%%%%%%%%%%
\section{Parte A: Generación de Texto}

\subsection{Datos}

En esta sección se explica el proceso de recopilación y limpieza de datos para entrenar los modelos de generación de texto utilizando letras de canciones.

\subsubsection{Recopilación de datos}

Se utilizaron dos scripts de Python para scrapear las letras de las canciones de un artista específico desde la plataforma \textit{Genius}. El primer script (\texttt{scrape\_songs\_alpaca.py}) realiza las siguientes tareas:

\begin{itemize}
    \item Autenticación en la API de Genius.
    \item Búsqueda de canciones (URLs) del artista.
    \item Extracción de las letras de las canciones.
    \item Limpieza y preprocesamiento de las letras en formato Alpaca para Fine-Tuning.
    \item Almacenamiento de las letras en un archivo de texto.
\end{itemize}

El segundo script (\texttt{preprocess\_scraped\_songs.py}) se encarga de procesar la salida del primer script para generar un archivo .txt con el formato adecuado para el entrenamiento de los modelos RNN/LSTM/GRU.

La selección de artistas y el número de canciones a scrapear se encuentran configuradas directamente en el código del primer script. En este caso, elegí a \textit{Kendrick Lamar}, \textit{Kanye West} y \textit{Jay-Z} debido al género compartido \textit{Hip-Hop/Rap} y la riqueza lírica de sus canciones. El número de canciones por artista se estableció en 100, lo que da un total aproximado de 300 canciones.

La ejecución de ambos scripts se realiza desde la terminal con la serie de comandos:

\begin{center}
\texttt{python ./codigo/generative/scraping/scrape\_songs\_alpaca.py} \\
\texttt{python ./codigo/generative/scraping/preprocess\_scraped\_songs.py}
\end{center}

\textbf{Nota:} Es importante asegurarse de tener la clave de API de Genius configurada en un archivo \texttt{.env} en el mismo directorio que el script. Ejemplo: \texttt{GENIUS\_API\_TOKEN="tu\_token\_aqui"}.

Al finalizar la ejecución, obtenemos distintos archivos para ambos formatos de salida particionados en conjuntos de entrenamiento y prueba. Además, también se generan los diccionarios necesarios para el preprocesamiento de texto en los modelos RNN/LSTM/GRU, y estadísticas del conjunto de datos. Los archivos generados son almacenados en el directorio \texttt{./data/text\_gen/} y son los siguientes:
\begin{itemize}
    \item \texttt{train\_lyrics\_alpaca.json} y
    \item  \texttt{test\_lyrics\_alpaca.json} (formato Alpaca).
    \item \texttt{train\_lyrics.txt} y
    \item \texttt{test\_lyrics.txt} (formato para RNN/LSTM/GRU).
    \item \texttt{vocab\_char.json} y
    \item \texttt{vocab\_word.json} (diccionarios de caracteres y palabras).
    \item \texttt{dataset\_info.json} (estadísticas del conjunto de datos).
\end{itemize}



\subsubsection{Limpieza y preprocesamiento del texto}

Genius provee las letras en formato HTML dentro de \texttt{lyrics containers}, los cuales son etiquetas con el atributo \texttt{data-lyrics-container="true"}. El script extrae el texto de estas etiquetas y realiza las siguientes operaciones de limpieza:

\begin{itemize}
    \item Eliminación de etiquetas HTML.
    \item Eliminación de líneas en blanco y espacios innecesarios.
    \item Crea cada registro en formato Alpaca para el fine-tuning.
\end{itemize}


He decidido no eliminar las anotaciones de las canciones (como [Coro], [Verso 1], etc.) ya que pueden proporcionar contexto adicional al modelo durante el entrenamiento, además de que pueden ser útiles para la generación de texto (haciendo que el modelo también genere anotaciones). Además, se conservaron los metadatos como nombre de la canción y artista, ya que pueden ser útiles para futuras referencias o análisis.

Al realizar Fine-Tuning sobre el LLM, es conveniente proporcionar contexto adicional (y acorde) al modelo. Debido a que en este caso nos decidimos por utilizar un modelo instructivo, el formato Alpaca es adecuado para este propósito. Este formato contiene campos para la instrucción, entrada y salida, lo que permite al modelo aprender a generar texto basado en instrucciones específicas de un solo turno.
\begin{verbatim}
{
    "instruction": "<INSTRUCCIÓN>",
    "input": "<ENTRADA>",
    "output": "<SALIDA>"
}
\end{verbatim}

donde:
\begin{itemize} 
    \item \texttt{<INSTRUCCIÓN>} es una cadena que indica la tarea a realizar, en este caso: ``You are a hip-hop artist, helping people write lyrics.'' o variantes.
    \item \texttt{<ENTRADA>} es una cadena que proporciona contexto adicional, en este caso: ``Help me write a song in the style of \texttt{<ARTISTA>}.''
    \item \texttt{<SALIDA>} es la letra de la canción.
\end{itemize}

Por otro lado, al utilizar modelos RNN/LSTM/GRU basta con simplemente tener las letras en texto plano, ya que estos modelos no requieren el mismo nivel de contexto que los LLMs. Sin embargo, es importante asegurarse de que el texto esté limpio y bien formateado para evitar problemas durante el entrenamiento. El script de preprocesamiento convierte los textos de formato Alpaca a texto plano, asegurándose de que cada letra esté separada por los delimitadores de cada canción (En este caso \texttt{<|song\_start|>} y \texttt{<song\_end>}).


\subsection{Transformers}

En este trabajo se utilizó el modelo \texttt{unsloth/mistral-7B-instruct-v0.3-bnb-4bit} como base para el fine-tuning. Este modelo es una variante del modelo creado por Mistral: Mistral-7B \cite{jiang2023mistral7b}, y optimizado por \textit{Unsloth} para tareas de instrucción y cuantizado a 4 bits para reducir el tamaño del modelo.

El framework utilizado para el fine-tuning fue \texttt{transformers} de Hugging Face, junto con \texttt{peft} para la implementación de Low-Rank Adaptation (LoRA). La elección de LoRA se debe a su eficiencia en términos de memoria y tiempo de entrenamiento, permitiendo adaptar grandes modelos preentrenados con un costo computacional reducido \cite{hu2022lora}.

\textbf{Tokenizador y representación.} El modelo base emplea un tokenizador sub‐palabras basado en SentencePiece con un vocabulario de aproximadamente 32K tokens \cite{kudo2018sentencepiece}. Este enfoque permite manejar adecuadamente la variabilidad léxica propia de letras de rap (slang, contracciones y variaciones estilísticas) sin inflar excesivamente el vocabulario. No se realizó entrenamiento de un tokenizador nuevo, ya que el objetivo fue un ajuste fino instructivo y no la adaptación desde cero a un dominio léxico extremadamente distinto.

\textbf{Formato de datos.} Cada ejemplo del conjunto de entrenamiento se construyó en formato tipo \textit{Alpaca}: \texttt{(instruction, input, output)} y posteriormente convertido a un \emph{chat template} estilo \texttt{alpaca} mediante \texttt{tokenizer.apply\_chat\_template}. Para cada canción se genera típicamente un par instrucción–respuesta, donde la instrucción solicita la generación de letras en el estilo de un artista específico y la salida contiene la letra correspondiente. Tras el mapeo, el campo final consumido por el entrenador es un único texto ya formateado.

\textbf{Tamaño del conjunto.} El corpus curado contiene 239 ejemplos de entrenamiento y 60 de prueba (correspondientes a canciones únicas). Con un tamaño de lote efectivo de 16 secuencias (lote por dispositivo = 2, acumulación de gradientes = 8), el número de pasos por época es:
\[
\text{steps/epoch} = \lceil 239 / 16 \rceil = 15.
\]
Se entrenaron 3 épocas, resultando en 45 pasos totales (se almacenaron checkpoints intermedios consistentes con este valor). Esto refleja un escenario de ajuste fino ligero, más orientado a estilo que a aprendizaje exhaustivo.

\textbf{Longitud de secuencia.} Se fijó \texttt{max\_seq\_length = 2048}. Dado que las letras individuales rara vez exceden este límite, se deshabilitó el \emph{packing} (\texttt{packing=False}) para evitar mezclar canciones dentro de la misma secuencia y preservar coherencia estructural (secciones como [Verse], [Chorus], etc.).

\textbf{Configuración LoRA.} Se aplicó LoRA con \texttt{r = 32}, \texttt{lora\_alpha = 64}, \texttt{lora\_dropout = 0.0} sobre los módulos de proyección de atención y MLP: \texttt{q\_proj, k\_proj, v\_proj, o\_proj, gate\_proj, up\_proj, down\_proj}. Suponiendo dimensiones del modelo base (d\_model = 4096, tamaño intermedio = 14336, 32 capas), el número aproximado de parámetros entrenables añadidos por LoRA es:
\[
\text{por capa} = 4 \times r (4096 + 4096) + 3 \times r (4096 + 14336) = 4(32 \cdot 8192) + 3(32 \cdot 18432) \approx 2{,}818{,}048,
\]
\[
\text{total} \approx 32 \times 2{,}818{,}048 \approx 90.2\text{M},
\]
lo que representa ~1.2\% de los =7.3B parámetros totales, pero concentra toda la capacidad de adaptación. Estos pesos LoRA se mantienen en precisión media (FP16/BF16) mientras los pesos base permanecen cuantizados a 4 bits \cite{dettmers2023qloraefficientfinetuningquantized}.

\textbf{Hiperparámetros de entrenamiento.} Se utilizaron: tasa de aprendizaje \texttt{2e-4} (scheduler lineal con calentamiento de 5 pasos), \texttt{epochs = 3}, \texttt{per\_device\_train\_batch\_size = 2}, \texttt{gradient\_accumulation\_steps = 8}, \texttt{weight\_decay = 0.01}, optimizador \texttt{AdamW 8-bit} \cite{dettmers2022optimizers}, y semilla \texttt{3407}. El tamaño de lote efectivo resultante es 16 ejemplos. No se aplicó regularización adicional (\texttt{lora\_dropout = 0}).

\textbf{Gestión de memoria y hardware.} El uso combinado de cuantización 4-bit + LoRA reduce el pico de VRAM necesario a un rango que típicamente cabe en una sola GPU de 12–16 GB manteniendo la ventana de contexto de 2048. El script registra memoria reservada y máxima por dispositivo antes y después del entrenamiento. 

\textbf{Hardware utilizado.} El entrenamiento se realizó en un nodo con 2 GPUs NVIDIA RTX TITAN (24 GB VRAM cada una), 128 GB RAM y 24 núcleos de CPU Intel Xeon Silver 4214 (2.2 GHz). Equipo proporcionado por el Laboratorio de Supercómputo del Bajío (Lab-SB) del CIMAT. 

\subsubsection{Evaluación de la Generación de Texto}

\textbf{Perplejidad.} La \emph{perplejidad} (PPL) es una métrica estándar para evaluar modelos de lenguaje, que mide qué tan bien un modelo predice una muestra. Matemáticamente, la perplejidad se define como la exponencial de la entropía cruzada promedio por token:
\[
\text{PPL} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_{<i})\right),
\]
donde \(N\) es el número total de tokens en el conjunto de evaluación, y \(P(w_i | w_{<i})\) es la probabilidad condicional del token \(w_i\) dado el contexto previo \(w_{<i}\). Una perplejidad más baja indica un mejor rendimiento del modelo, ya que sugiere que el modelo está menos "sorprendido" por los datos de evaluación.

Para evaluar el impacto del fine-tuning, se comparó la perplejidad del modelo ajustado con la del modelo base \texttt{mistral-7B-instruct-v0.3-bnb-4bit} sin ajuste fino. Esto permite cuantificar la mejora en la capacidad predictiva del modelo tras el entrenamiento con las letras de canciones. El código para calcular la perplejidad es proporcionado por la misma biblioteca \texttt{Unsloth}, se encuentra en \texttt{codigo/generative/mistral/utils/perplexity\_unsloth.py}. La lógica fue adaptada en el script \texttt{calculate\_perplexity.py} para evaluar ambos modelos (base y fine-tuned) sobre el conjunto de prueba.

El resultado de la evaluación de perplejidad en el conjunto de prueba es el siguiente:
\begin{itemize}
    \item Perplejidad del modelo base (sin fine-tuning): 9.35.
    \item Perplejidad del modelo fine-tuned: 7.20.
    \item Mejora relativa: \((9.35 - 7.20) / 9.35 \times 100\% \approx 22.93\%\).
    \item Interpretación: La reducción significativa en la perplejidad indica que el modelo fine-tuned ha mejorado su capacidad para predecir las letras de canciones, adaptándose mejor al estilo y vocabulario específico del dominio. Esto fue logrado a pesar del tamaño relativamente pequeño del conjunto de datos de entrenamiento, demostrando la eficacia del enfoque de fine-tuning con LoRA en un modelo cuantizado.
\end{itemize}

\textbf{Muestras generadas.} 

Se generaron varias muestras de texto utilizando el modelo fine-tuned para evaluar su capacidad de generación. Todas las muestras se generaron con los siguientes parámetros en la inferencia, por mostrar un mejor balance entre coherencia y diversidad:
\begin{itemize}
    \item \texttt{max\_new\_tokens = 256}. Representa la longitud máxima de la secuencia generada.
    \item \texttt{temperature = 0.8} (0.7 para Kendrick Lamar). Controla la aleatoriedad de la generación; valores más bajos hacen que el modelo sea más conservador.
    \item \texttt{top\_p = 0.9}. Utiliza muestreo de núcleo para limitar la selección de tokens a los más probables.
    \item \texttt{repetition\_penalty = 1.3}. Penaliza la repetición de tokens para fomentar la diversidad en la generación.
\end{itemize}

A continuación se presentan algunos ejemplos representativos:

\textbf{Ejemplo 1: Letra generada en el estilo de Jay-Z}

En este ejemplo, se le pidió al modelo que generara una letra en el estilo de Jay-Z mediante el prompt:

\begin{verbatim}
    You're a hip-hop artist, create song lyrics. 
    Write hip-hop lyrics in the style of Jay-Z.
\end{verbatim}

Y se obtuvo la siguiente salida:

\begin{verbatim}
    [Intro]
    Uh huh uh (yeah)
    Yeah yeah you know my name is Hov', right? Uhh
    I got this little thing called ``The Blueprint''
    And I just want to run through some shit for y'all tonight
    Just have fun with it
    (Hands up!)

    [Verse 1]
    Yo, yo, yo, yo, whattup! It's your boy Shawn
    Back on that bullshit 'cause niggas ain't learn
    Last time out I burned down the barn
    Now I'm back and I might burn down another one
    What do you wanna bet? You can put all ya chips in
    If he don't win then we all gonna be broke again
    But if he does win then we gon' get richer than rich men
    So place your bets now before they stop taking them
    See me as an investment like Berkley or Microsoft
    Put ten dollars into me baby watch how far it goes
    Watch how fast it grows
\end{verbatim}

Podemos analizar la letra generada entre pilares del estilo de cada artista:

\begin{itemize}
    \item \textbf{Persona y Autenticidad.} El intro es muy representativo del estilo de Jay-Z, utilizando \textit{ad-libs} (Uh, uh, yeah) y directamente referenciando su nickname (Hov'). También hace referencia a su álbum ``The Blueprint'', lo que refuerza su identidad como artista. El uso de ``Your boy Shawn'' es también auténtico.
    
    \item \textbf{Temas y Contenido.} La letra realiza una metáfora sobre invertir en sí mismo como un artista, lo cual es un tema recurrente en la música de Jay-Z.

    \item \textbf{Rimas y Fluidez.} La letra presenta un esquema de rima consistente (AABB), aunque la estructura es predecible y carece de complejidad y expertiz de Jay-Z.
\end{itemize}

\textbf{Ejemplo 2: Letra generada en el estilo de Kanye West}

Nuevamente, se le pidió al modelo la generación de una letra, ahora en el estilo de Kanye West. Esto fue mediante el prompt:

\begin{verbatim}
    You're a hip-hop artist, create song lyrics. 
    Write hip-hop lyrics in the style of Kanye West.
\end{verbatim}


Y se obtuvo la siguiente salida:

\begin{verbatim}
    [Intro]
    I know He won’t forsake me
    He won’t leave me
    No matter what I do
    And even if I go through hell tonight
    He will still be God when it’s all over with
    (God is) (God is)

    [Verse 1]
    They say build your own
    I said fuck that, we gon' buy our way out
    We ain't finna wait 'til they give us something to celebrate
    So let's get this bread
    Fuck up school and just drop out
    But you can't drop out without dropping some knowledge first
    Let me teach y'all how to make yourself rich
    How to stay fly while staying righteous
    First thing you need is faith as big as Mount Rushmore
    Then take it from here

    [Chorus]
    Jesus saved my soul
    Now I drive Rollies and Bentleys
    Bought myself a couple chains
    Spendin' money like I don't owe Him nothin'
    Everybody want heaven but nobody wanna die
    Nobody wanna cry no more
    That means everybody wanna live forever
    Not ready to meet Your maker
\end{verbatim}

\begin{itemize}
    \item \textbf{Persona y Autenticidad.} La letra captura la dualidad central de la persona de Kanye West: la del predicador y el materialista. El intro con temática góspel y las referencias a Dios son muy representativas de su era en los finales de los 10's con el album \textit{Jesus is King}. Esto se contrasta inmediatamente con un tono provocador y egocéntrico (Let me teach y'all'', fuck that''), lo cual es sumamente auténtico a su estilo.

    \item \textbf{Temas y Contenido.} El modelo representa exitosamente la mezcla de temas de Kanye. Se abordan la religión y la fe (``Jesus saved my soul'', ``faith as big as Mount Rushmore'') en conflicto directo con el materialismo extremo y el lujo (``drive Rollies and Bentleys''). Incluye también aforismos y comentarios sociales simplificados, como la línea ``Everybody want heaven but nobody wanna die'', un recurso muy común en sus letras.

    \item \textbf{Rimas y Fluidez.} Kanye a menudo prioriza el mensaje y la emoción sobre la complejidad de la rima. La letra generada refleja esto: el esquema de rima es inconsistente y conversacional. Se apoya más en la cadencia y en la fuerza de las declaraciones que en patrones de rima complejos, lo que resulta en un flujo que se asemeja a un sermón o una declaración apasionada, muy fiel a su estilo de entrega.
\end{itemize}


\textbf{Ejemplo 3: Letra generada en el estilo de Kendrick Lamar}

Esta vez se le pidió al modelo generar una letra para una canción de Hip-Hop al estilo de Kendrick Lamar, mediante el prompt:
    
\begin{verbatim}
    You're a hip-hop artist, create song lyrics. 
    Write hip-hop lyrics in the style of Kendrick Lamar.
\end{verbatim}

Y se obtuvo la siguiente salida:

\begin{verbatim}
    [Intro]
    I don’t know what they want from me! I gave them my life
    They wanna see some type of drama? Here it is
    (Bitch)

    [Verse 1]
    What up homie? What you writing about today? (Today?)
    Writing 'bout these niggas that be hating on me (Huh?)
    And all this fake love that surrounds me (Uh)
    All this bullshit that goes around me (Yeah)
    It gets tiring sometimes when you trying to stay positive
    But negative energy always surrounding you
    So how can we live with no stress and anxiety?
    When everybody got something bad to say bout you
    Everybody wants something but ain't willing to give nothin' back
    Nobody really cares if you win or lose at shit
    If your name isn't mentioned then nobody gon' mention you
    Even though you did more than most people ever done
    That'll make you feel like you worthless
    Like you never won
    Never been number one
    Always second place even when you first
    How do you deal with that kind of pressure? How do you cope?
    Knowin' every move you make will affect your whole family tree
    Your friends, your enemies, your lovers, haters, neighbors
    The world, society, everyone expect perfection
    From an imperfect person
\end{verbatim}
\begin{itemize}
    \item \textbf{Persona y Autenticidad.} Captura a la perfección la persona introspectiva, ansiosa y abrumada que define gran parte de la obra reciente de Kendrick Lamar (DAMN y MM\&BS). La letra se lee como un monólogo interno o una sesión de terapia, explorando la presión de la fama de una manera vulnerable y conflictiva, lo cual es una característica central de su estilo.

    \item \textbf{Temas y Contenido.} Los temas abordados son extremadamente fieles al universo lírico de Kendrick. Se centra en la salud mental (``stress and anxiety''), el peso de las expectativas (``everyone expect perfection''), la falsedad en la industria (``fake love that surrounds me''), y la responsabilidad generacional (``every move you make will affect your whole family tree''). Este enfoque en el conflicto interno y la carga del liderazgo es mucho más representativo que la simple bravuconería.

    \item \textbf{Rimas y Fluidez.} El modelo imita exitosamente la fluidez conversacional y casi de "spoken word" de Kendrick. En lugar de adherirse a un esquema de rima estricto, prioriza el flujo de conciencia y la carga emocional del mensaje. Las líneas son largas y discursivas, construyendo una tensión que culmina en la lista final (``Your friends, your enemies...''). Esta estructura, que favorece el ritmo narrativo sobre la rima perfecta, es una técnica clave en el repertorio de Kendrick.
\end{itemize}

En los tres ejemplos, el modelo fine-tuned demuestra una capacidad notable para capturar los estilos únicos de cada artista, tanto en términos de contenido temático como la estructura lírica. Aunque no alcanza la complejidad y profundidad de los artistas originales, especialmente en términos de rima y metáforas, el modelo genera letras coherentes y estilísticamente apropiadas para los artistas. Además, el modelo respetó en su mayoría la estructura de las letras basandose en los datos de entrenamiento (por ejemplo, siempre inicio con el [Intro], [Verse 1], etc.). Esto sugiere que el enfoque de fine-tuning con un conjunto de datos relativamente pequeño pero bien curado puede ser efectivo para adaptar un modelo de lenguaje grande a tareas creativas específicas como la generación de letras de canciones.

\newpage
\subsection{RNN Simple}
Las RNN simples constituyen la forma canónica de modelar dependencias secuenciales mediante un estado oculto recurrente. Cada paso de tiempo aplica una transformación no lineal sobre la combinación del embedding del token actual y el estado oculto previo. Sin embargo, su profundidad temporal efectiva se ve limitada por el \emph{desvanecimiento} y la \emph{explosión} del gradiente, lo cual dificulta capturar dependencias de largo alcance en secuencias lingüísticas (estructuras repetitivas, coherencia temática, progresión narrativa, etc.).

	\textbf{Tokenización y representación.} Se entrenaron dos variantes: a nivel carácter y a nivel palabra. El vocabulario carácter contiene 157 símbolos (incluyendo saltos de línea, puntuación y caracteres Unicode presentes en las letras). El vocabulario palabra contiene 19{,}107 tokens (palabras, formas con apóstrofos y algunos préstamos multilingües). Cada token se proyecta a un embedding de dimensión 128.

	\textbf{Formato de datos.} El corpus en texto plano se segmenta en ventanas deslizantes de longitud fija \texttt{seq\_length = 100}. Para cada ventana de 100 tokens (o caracteres) se predice el siguiente token. No se mezcla información entre canciones más allá de la concatenación lineal del corpus, pero se preservan delimitadores estructurales (\texttt{<|song\_start|>} / \texttt{<song\_end>}), lo que provee señales débiles de segmentación.

	\textbf{Tamaño del conjunto.} El conjunto de entrenamiento contiene 820{,}993 caracteres (nivel de carácter) y 157{,}910 palabras (nivel de palabra). El conjunto de prueba contiene 214{,}957 caracteres y 41{,}889 palabras, respectivamente. El número de secuencias efectivas por nivel se calculan como (longitud$-$100). Con \texttt{batch\_size = 64}, esto produce aproximadamente 12.8K batches por época (carácter) y 2.5K batches por época (palabra), lo que impacta la relación señal–ruido de la estimación del gradiente (más estable en la variante palabra, más estocástica en la variante carácter).

	\textbf{Arquitectura.} La celda recurrente implementa la actualización clásica:
\[
h_t = \tanh(W_{xh} x_t + W_{hh} h_{t-1} + b),\quad y_t = W_{hy} h_t + c,
\]
donde $x_t \in \mathbb{R}^{128}$, $h_t \in \mathbb{R}^{256}$. Se emplean 2 capas apiladas (\texttt{num\_layers = 2}) con \texttt{dropout = 0.3} entre capas (no entre pasos temporales internos de cada capa).

\textbf{Capacidad y parámetros.} Aproximadamente:
\begin{itemize}
    \item Variante palabra: $\sim 7.59$M parámetros entrenables (embeddings $\approx$2.45M, núcleo recurrente $\approx$0.23M, proyección de salida $\approx$4.91M).
    \item Variante carácter: $\sim 0.29$M parámetros (la reducción proviene del vocabulario compacto).
\end{itemize}
Esto ilustra la fuerte asimetría de capacidad inducida por el tamaño del vocabulario y la capa de salida softmax.

	\textbf{Hiperparámetros de entrenamiento.} Se usó el optimizador Adam (\texttt{lr = 2e-3}), 20 épocas, \texttt{batch\_size = 64}, \texttt{seq\_length = 100}, \texttt{gradient clipping = 5.0}. La función de pérdida es la entropía cruzada. Un scheduler \texttt{ReduceLROnPlateau} reduce la tasa de aprendizaje con factor 0.5 tras 2 épocas sin mejora significativa (\texttt{patience = 2}). La pérdida se reporta en escala log-perplexity y puede re–expresarse como $\text{PPL} = e^{\mathcal{L}}$. Se habilitó opción de \emph{early stopping} configurable.

	\textbf{Regularización y estabilidad.} El \emph{dropout} inter–capas y el \emph{gradient clipping} controlan, respectivamente, sobre–ajuste y explosión de gradientes. Aún así, las limitaciones estructurales (sin mecanismos de compuertas ni memoria explícita) reducen su capacidad para retener contexto semántico más allá de decenas de pasos.

	\textbf{Hardware utilizado.} El mismo entorno descrito en la subsección de Transformers: 2× NVIDIA RTX TITAN (24 GB VRAM), 128 GB RAM y 24 núcleos CPU Intel Xeon Silver 4214, utilizando \texttt{PyTorch} para la implementación y entrenamiento.


\newpage
\subsection{LSTM}
Las LSTM introducen compuertas que mitigan el desvanecimiento del gradiente y permiten retener información relevante a través de pasos temporales largos. La celda mantiene un estado de memoria $c_t$ separado del estado oculto $h_t$, modulando flujos de información mediante puertas sigmoides.

	\textbf{Tokenización y representación.} Se emplean las mismas dos granularidades (carácter y palabra) con embeddings de dimensión 128 inicializados aleatoriamente y aprendidos de forma conjunta. No se reutilizó el tokenizador sub‐palabras del modelo Transformer dado que aquí el foco es analizar arquitecturas recurrentes “clásicas”.

	\textbf{Formato de datos.} Idéntico al de la RNN simple: ventanas deslizantes de longitud 100, objetivo el token siguiente. La preservación de delimitadores de canción actúa como débil frontera semántica, y se espera que la LSTM capture patrones de estructura (introducciones, coros) con mayor robustez que la RNN simple.

	\textbf{Tamaño del conjunto.} Igual a la variante anterior; esto permite comparaciones controladas de arquitectura.

	\textbf{Arquitectura.} Dos capas LSTM (\texttt{hidden\_dim = 256}, \texttt{num\_layers = 2}) con \texttt{dropout = 0.3} entre capas. Ecuaciones de la celda (omitiendo subíndices de capa):
\[
\begin{aligned}
i_t &= \sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i),\\
f_t &= \sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f),\\
g_t &= \tanh(W_{xg} x_t + W_{hg} h_{t-1} + b_g),\\
o_t &= \sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o),\\
c_t &= f_t \odot c_{t-1} + i_t \odot g_t,\\
h_t &= o_t \odot \tanh(c_t).
\end{aligned}
\]

\textbf{Capacidad y parámetros.} Aprox.:
\begin{itemize}
    \item Variante palabra: $\sim 8.28$M parámetros (el incremento sobre RNN simple proviene del factor de 4 en matrices de compuertas).
    \item Variante carácter: $\sim 0.98$M parámetros.
\end{itemize}
El salto de capacidad frente a la RNN simple se concentra en las matrices de compuertas, incrementando la expresividad temporal.

\textbf{Hiperparámetros de entrenamiento.} Se mantuvieron los mismos valores globales para favorecer comparabilidad: \texttt{Adam}, 20 épocas de entrenamiento, \texttt{batch\_size = 64}, \texttt{seq\_length = 100}, \texttt{clip = 5.0}, scheduler adaptativo sobre la pérdida de validación y potencial \emph{early stopping}. La métrica primaria interna es la entropía cruzada; la perplexity derivada sirve para contrastes posteriores (no reportada aquí).

\textbf{Regularización y estabilidad.} El mecanismo de compuertas (en particular $f_t$) estabiliza el flujo de gradientes y atenúa el desvanecimiento típico. El \emph{dropout} entre capas actúa como regularizador estructural, y el \emph{gradient clipping} asegura control frente a ráfagas de magnitud.

\textbf{Ventajas observadas conceptualmente.} Mayor retención de contexto largo (estados temáticos, continuidad de narrador) y mejor manejo de secciones donde la progresión semántica se apoya en repeticiones moduladas (p. ej. estribillos). 

\textbf{Hardware utilizado.} Idéntico al descrito previamente (2× RTX TITAN, etc.), con ejecución en \texttt{PyTorch}.


\newpage
\subsection{GRU}
Las GRU (Gated Recurrent Units) simplifican la estructura de la LSTM fusionando algunas compuertas y eliminando el estado de memoria separado. Mantienen capacidad para mitigar el desvanecimiento del gradiente con menor costo parametrizable, lo que puede traducirse en entrenamientos más rápidos y menor riesgo de sobre–ajuste en conjuntos moderados.

	\textbf{Tokenización y representación.} Se reutiliza idéntica configuración (carácter y palabra, embeddings de 128). La comparación GRU vs LSTM bajo el mismo espacio de embeddings permite aislar el impacto de la diferencia arquitectónica.

	\textbf{Formato de datos.} Igual que en las dos arquitecturas previas (ventanas de 100 tokens → predicción del siguiente). Esto garantiza que cualquier diferencia empírica futura (perplejidad, estabilidad) se atribuya al diseño de la celda y no a variaciones de entrada.

	\textbf{Arquitectura.} Dos capas GRU de 256 unidades con \texttt{dropout = 0.3} entre capas. Las ecuaciones de la celda:
\[
\begin{aligned}
z_t &= \sigma(W_{xz} x_t + W_{hz} h_{t-1} + b_z) & \text{(puerta de actualización)}\\
r_t &= \sigma(W_{xr} x_t + W_{hr} h_{t-1} + b_r) & \text{(puerta de reinicio)}\\
	ilde{h}_t &= \tanh(W_{xh} x_t + W_{hh} (r_t \odot h_{t-1}) + b_h) & \text{(candidato)}\\
h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t.
\end{aligned}
\]

	extbf{Capacidad y parámetros.} Aproximadamente:
\begin{itemize}
    \item Variante palabra: $\sim 8.05$M parámetros (intermedio entre RNN simple y LSTM; 3 grupos de matrices en lugar de 1 o 4).
    \item Variante carácter: $\sim 0.75$M parámetros.
\end{itemize}
La reducción frente a LSTM puede favorecer menor sobre–ajuste en dominios con variabilidad estilística pero tamaño moderado.

	\textbf{Hiperparámetros de entrenamiento.} Se conservaron los mismos (\texttt{Adam}, \texttt{lr = 2e-3}, \texttt{epochs = 20}, \texttt{batch\_size = 64}, \texttt{seq\_length = 100}, clipping 5.0, scheduler adaptativo). Esta homogeneidad facilita comparaciones directas posteriores de eficiencia y convergencia.

	\textbf{Regularización y estabilidad.} El diseño de la puerta de actualización $z_t$ actúa como interpolador adaptativo entre memoria previa y nuevo contenido, reduciendo la necesidad de un estado separado tipo $c_t$. El \emph{dropout} inter–capas y el clipping complementan el control de generalización y estabilidad numérica.

	\textbf{Consideraciones comparativas.} Conceptualmente, la GRU ofrece un compromiso: menos parámetros que LSTM, más expresión que la RNN simple. En escenarios con ventana de contexto moderada (100 tokens) y un dominio con repeticiones estilísticas claras (estructuras líricas), puede captar patrones de transición (Intro → Verso, Verso → Chorus) sin incurrir en el costo completo de las LSTM.

	\textbf{Hardware utilizado.} Mismo entorno GPU/CPU ya descrito; implementación en \texttt{PyTorch} reutilizando la canalización de datos y bucles de entrenamiento comunes.


\newpage
\bibliographystyle{unsrt} % Elige un estilo (otros: abbrvnat, unsrtnat, etc.)
\bibliography{bib} % Indica el nombre de tu archivo .bib (sin la extensión)


\end{document}
