% !TeX recipe = Rtex
% Optimizado para compilación rápida
\documentclass[paper=letter, fontsize=11pt, draft=false]{scrartcl}

% Modifications to layout
\usepackage[shortlabels]{enumitem} % Incisos
\def\code#1{\texttt{#1}} % \code{} for monospaced text
\newcommand{\RNum}[1]{\footnotesize\uppercase\expandafter{\romannumeral #1\relax\normalsize}} % Roman numbers

\usepackage{subcaption} % 2x2 graphs
\usepackage{mwe}
\usepackage{float} % [H] in graphics
\usepackage[hidelinks]{hyperref}  % Hipervínculos en la TOC

\usepackage{booktabs,siunitx,listings}
\usepackage[most]{tcolorbox}
\tcbuselibrary{theorems}
\usepackage{cleveref}

% Typography and layout packages
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage[spanish,es-nodecimaldot]{babel} % Language and hyphenation
\usepackage{amsmath, amsfonts, amsthm, amssymb} % Math packages
\newtheorem{definition}{Definición} % definition
\usepackage{fancyvrb}
\usepackage{sectsty} % Customize section commands
\usepackage{geometry} % Modify margins
\usepackage{titlesec} % Customize section titles
\geometry{margin=3cm,top=2.5cm,bottom=2.5cm} % Simplified geometry
\allsectionsfont{\centering \normalfont\scshape} % Center and style section titles

% Header and footer customization
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead[L]{\slshape} % Remove section title from header
\fancyhead[C]{} % Header center
\fancyhead[R]{\thepage} % Header right with page number
\fancyfoot[L]{} % Footer left
\fancyfoot[C]{} % Footer center
\fancyfoot[R]{\small \slshape Gustavo Hernández Angeles} % Footer right
\renewcommand{\headrulewidth}{0.4pt} % Header rule
\renewcommand{\footrulewidth}{0.4pt} % Footer rule
\setlength{\headheight}{14.5pt} % Header height

% Paragraph settings
\setlength\parindent{0pt}
\setlength{\parskip}{1ex}

% Section spacing
\titlespacing*{\section}{0cm}{0.50cm}{0.25cm}

% --- Theorems, lemma, corollary, postulate, definition ---
\definecolor{Pantone209C}{HTML}{64293e}

\newcounter{problemcounter}

\numberwithin{equation}{section} % Number equations within problems
\numberwithin{figure}{section} % Number figures within problems
\numberwithin{table}{section} % Number tables within problems
\numberwithin{subsection}{section} 

\newtcbtheorem[auto counter]{problem}{Ejercicio}{
    enhanced,
    breakable,
    colback = gray!5,
    colframe = gray!5,
    boxrule = 0.5pt,
    sharp corners,
    borderline west = {2mm}{0mm}{Pantone209C},
    fonttitle = \bfseries\sffamily,
    coltitle = Pantone209C,
    drop fuzzy shadow,
    parbox = false,
    before skip = 3ex,
    after skip = 3ex
}{problem}
\makeatletter
\renewenvironment{problem}[2][]{%
    \refstepcounter{problemcounter}%
    \addcontentsline{toc}{section}{\protect\numberline{\theproblemcounter}Ejercicio \theproblemcounter: #2}%
    \begin{tcolorbox}[
        enhanced,
        breakable,
        colback = gray!5,
        colframe = gray!5,
        boxrule = 0.5pt,
        sharp corners,
        borderline west = {2mm}{0mm}{Pantone209C},
        fonttitle = \bfseries\sffamily,
        coltitle = Pantone209C,
        drop fuzzy shadow,
        parbox = false,
        before skip = 3ex,
        after skip = 3ex,
        title={Ejercicio \theproblemcounter: #2}
    ]
}{%
    \end{tcolorbox}
}
\makeatother

\tcbuselibrary{skins, breakable}
% Custom command for a horizontal rule
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 

% Custom section titles with numbering
\titleformat{\section}
{\normalfont\Large\bfseries}{\thesection}{1em}{}

\titleformat{\subsection}
{\normalfont\large\bfseries}{\thesubsection}{1em}{}

\titleformat{\subsubsection}
{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

% Title and author
\title{	
    \begin{center}
        \includegraphics[width=3cm]{figure/template/cimat-logo.png} % Adjust size as needed
    \end{center}
    \vspace{0.5cm}
    \normalfont \normalsize 
    \textbf{\Large   Centro de Investigación en Matemáticas} \\
    \Large Unidad Monterrey \\ [25pt] 
    \horrule{1pt} \\[0.4cm] % Thin top horizontal rule
    \huge Análisis de Texto e Imágenes\\
    \Large Generación y Clasificación de Texto con Deep Learning\\ 
    \horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{\large Gustavo Hernández Angeles}    

\date{\normalsize\today} % Today's date

\begin{document}
\maketitle % Print the title
\thispagestyle{empty}
\newpage

\tableofcontents
\newpage



%%%%%%%%%%% PROBLEMA 1 %%%%%%%%%%%
\section{Parte A: Generación de Texto}

\subsection{Datos}

En esta sección se explica el proceso de recopilación y limpieza de datos para entrenar los modelos de generación de texto utilizando letras de canciones.

\subsubsection{Recopilación de datos}

Se utilizaron dos scripts de Python para scrapear las letras de las canciones de un artista específico desde la plataforma \textit{Genius}. El primer script (\texttt{scrape\_songs\_alpaca.py}) realiza las siguientes tareas:

\begin{itemize}
    \item Autenticación en la API de Genius.
    \item Búsqueda de canciones (URLs) del artista.
    \item Extracción de las letras de las canciones.
    \item Limpieza y preprocesamiento de las letras en formato Alpaca para Fine-Tuning.
    \item Almacenamiento de las letras en un archivo de texto.
\end{itemize}

El segundo script (\texttt{preprocess\_scraped\_songs.py}) se encarga de procesar la salida del primer script para generar un archivo .txt con el formato adecuado para el entrenamiento de los modelos RNN/LSTM/GRU.

La selección de artistas y el número de canciones a scrapear se encuentran configuradas directamente en el código del primer script. En este caso, elegí a \textit{Kendrick Lamar}, \textit{Kanye West} y \textit{Jay-Z} debido al género compartido \textit{Hip-Hop/Rap} y la riqueza lírica de sus canciones. El número de canciones por artista se estableció en 100, lo que da un total aproximado de 300 canciones.

La ejecución de ambos scripts se realiza desde la terminal con la serie de comandos:

\begin{center}
\texttt{python ./codigo/generative/scraping/scrape\_songs\_alpaca.py} \\
\texttt{python ./codigo/generative/scraping/preprocess\_scraped\_songs.py}
\end{center}

\textbf{Nota:} Es importante asegurarse de tener la clave de API de Genius configurada en un archivo \texttt{.env} en el mismo directorio que el script. Ejemplo: \texttt{GENIUS\_API\_TOKEN="tu\_token\_aqui"}.

Al finalizar la ejecución, obtenemos distintos archivos para ambos formatos de salida particionados en conjuntos de entrenamiento y prueba. Además, también se generan los diccionarios necesarios para el preprocesamiento de texto en los modelos RNN/LSTM/GRU, y estadísticas del conjunto de datos. Los archivos generados son almacenados en el directorio \texttt{./data/text\_gen/} y son los siguientes:
\begin{itemize}
    \item \texttt{train\_lyrics\_alpaca.json} y
    \item  \texttt{test\_lyrics\_alpaca.json} (formato Alpaca).
    \item \texttt{train\_lyrics.txt} y
    \item \texttt{test\_lyrics.txt} (formato para RNN/LSTM/GRU).
    \item \texttt{vocab\_char.json} y
    \item \texttt{vocab\_word.json} (diccionarios de caracteres y palabras).
    \item \texttt{dataset\_info.json} (estadísticas del conjunto de datos).
\end{itemize}



\subsubsection{Limpieza y preprocesamiento del texto}

Genius provee las letras en formato HTML dentro de \texttt{lyrics containers}, los cuales son etiquetas con el atributo \texttt{data-lyrics-container="true"}. El script extrae el texto de estas etiquetas y realiza las siguientes operaciones de limpieza:

\begin{itemize}
    \item Eliminación de etiquetas HTML.
    \item Eliminación de líneas en blanco y espacios innecesarios.
    \item Crea cada registro en formato Alpaca para el fine-tuning.
\end{itemize}


He decidido no eliminar las anotaciones de las canciones (como [Coro], [Verso 1], etc.) ya que pueden proporcionar contexto adicional al modelo durante el entrenamiento, además de que pueden ser útiles para la generación de texto (haciendo que el modelo también genere anotaciones). Además, se conservaron los metadatos como nombre de la canción y artista, ya que pueden ser útiles para futuras referencias o análisis.

Al realizar Fine-Tuning sobre el LLM, es conveniente proporcionar contexto adicional (y acorde) al modelo. Debido a que en este caso nos decidimos por utilizar un modelo instructivo, el formato Alpaca es adecuado para este propósito. Este formato contiene campos para la instrucción, entrada y salida, lo que permite al modelo aprender a generar texto basado en instrucciones específicas de un solo turno.
\begin{verbatim}
{
    "instruction": "<INSTRUCCIÓN>",
    "input": "<ENTRADA>",
    "output": "<SALIDA>"
}
\end{verbatim}

donde:
\begin{itemize} 
    \item \texttt{<INSTRUCCIÓN>} es una cadena que indica la tarea a realizar, en este caso: ``You are a hip-hop artist, helping people write lyrics.'' o variantes.
    \item \texttt{<ENTRADA>} es una cadena que proporciona contexto adicional, en este caso: ``Help me write a song in the style of \texttt{<ARTISTA>}.''
    \item \texttt{<SALIDA>} es la letra de la canción.
\end{itemize}

Por otro lado, al utilizar modelos RNN/LSTM/GRU basta con simplemente tener las letras en texto plano, ya que estos modelos no requieren el mismo nivel de contexto que los LLMs. Sin embargo, es importante asegurarse de que el texto esté limpio y bien formateado para evitar problemas durante el entrenamiento. El script de preprocesamiento convierte los textos de formato Alpaca a texto plano, asegurándose de que cada letra esté separada por los delimitadores de cada canción (En este caso \texttt{<|song\_start|>} y \texttt{<song\_end>}).


\subsection{Transformers}

\subsubsection{Mistral 7B Instruct v0.3}

En este trabajo se utilizó el modelo \texttt{unsloth/mistral-7B-instruct-v0.3-bnb-4bit} como base para el fine-tuning. Este modelo es una variante del modelo creado por Mistral: Mistral-7B \cite{jiang2023mistral7b}, y optimizado por \textit{Unsloth} para tareas de instrucción y cuantizado a 4 bits para reducir el tamaño del modelo.

El framework utilizado para el fine-tuning fue \texttt{transformers} de Hugging Face, junto con \texttt{peft} para la implementación de Low-Rank Adaptation (LoRA). La elección de LoRA se debe a su eficiencia en términos de memoria y tiempo de entrenamiento, permitiendo adaptar grandes modelos preentrenados con un costo computacional reducido \cite{hu2022lora}.

\subsubsection{Configuración del Fine-Tuning}

\textbf{Tokenizador y representación.} El modelo base emplea un tokenizador sub‐palabras basado en SentencePiece con un vocabulario de aproximadamente 32K tokens \cite{kudo2018sentencepiece}. Este enfoque permite manejar adecuadamente la variabilidad léxica propia de letras de rap (slang, contracciones y variaciones estilísticas) sin inflar excesivamente el vocabulario. No se realizó entrenamiento de un tokenizador nuevo, ya que el objetivo fue un ajuste fino instructivo y no la adaptación desde cero a un dominio léxico extremadamente distinto.

\textbf{Formato de datos.} Cada ejemplo del conjunto de entrenamiento se construyó en formato tipo \textit{Alpaca}: \texttt{(instruction, input, output)} y posteriormente convertido a un \emph{chat template} estilo \texttt{alpaca} mediante \texttt{tokenizer.apply\_chat\_template}. Para cada canción se genera típicamente un par instrucción–respuesta, donde la instrucción solicita la generación de letras en el estilo de un artista específico y la salida contiene la letra correspondiente. Tras el mapeo, el campo final consumido por el entrenador es un único texto ya formateado.

\textbf{Tamaño del conjunto.} El corpus curado contiene 239 ejemplos de entrenamiento y 60 de prueba (correspondientes a canciones únicas). Con un tamaño de lote efectivo de 16 secuencias (lote por dispositivo = 2, acumulación de gradientes = 8), el número de pasos por época es:
\[
\text{steps/epoch} = \lceil 239 / 16 \rceil = 15.
\]
Se entrenaron 3 épocas, resultando en 45 pasos totales (se almacenaron checkpoints intermedios consistentes con este valor). Esto refleja un escenario de ajuste fino ligero, más orientado a estilo que a aprendizaje exhaustivo.

\textbf{Longitud de secuencia.} Se fijó \texttt{max\_seq\_length = 2048}. Dado que las letras individuales rara vez exceden este límite, se deshabilitó el \emph{packing} (\texttt{packing=False}) para evitar mezclar canciones dentro de la misma secuencia y preservar coherencia estructural (secciones como [Verse], [Chorus], etc.).

\textbf{Configuración LoRA.} Se aplicó LoRA con \texttt{r = 32}, \texttt{lora\_alpha = 64}, \texttt{lora\_dropout = 0.0} sobre los módulos de proyección de atención y MLP: \texttt{q\_proj, k\_proj, v\_proj, o\_proj, gate\_proj, up\_proj, down\_proj}. Suponiendo dimensiones del modelo base (d\_model = 4096, tamaño intermedio = 14336, 32 capas), el número aproximado de parámetros entrenables añadidos por LoRA es:
\[
\text{por capa} = 4 \times r (4096 + 4096) + 3 \times r (4096 + 14336) = 4(32 \cdot 8192) + 3(32 \cdot 18432) \approx 2{,}818{,}048,
\]
\[
\text{total} \approx 32 \times 2{,}818{,}048 \approx 90.2\text{M},
\]
lo que representa ~1.2\% de los =7.3B parámetros totales, pero concentra toda la capacidad de adaptación. Estos pesos LoRA se mantienen en precisión media (FP16/BF16) mientras los pesos base permanecen cuantizados a 4 bits \cite{dettmers2023qloraefficientfinetuningquantized}.

\textbf{Hiperparámetros de entrenamiento.} Se utilizaron: tasa de aprendizaje \texttt{2e-4} (scheduler lineal con calentamiento de 5 pasos), \texttt{epochs = 3}, \texttt{per\_device\_train\_batch\_size = 2}, \texttt{gradient\_accumulation\_steps = 8}, \texttt{weight\_decay = 0.01}, optimizador \texttt{AdamW 8-bit} \cite{dettmers2022optimizers}, y semilla \texttt{3407}. El tamaño de lote efectivo resultante es 16 ejemplos. No se aplicó regularización adicional (\texttt{lora\_dropout = 0}).

\textbf{Gestión de memoria y hardware.} El uso combinado de cuantización 4-bit + LoRA reduce el pico de VRAM necesario a un rango que típicamente cabe en una sola GPU de 12–16 GB manteniendo la ventana de contexto de 2048. El script registra memoria reservada y máxima por dispositivo antes y después del entrenamiento. 

\textbf{Hardware utilizado.} El entrenamiento se realizó en un nodo con 2 GPUs NVIDIA RTX TITAN (24 GB VRAM cada una), 128 GB RAM y 24 núcleos de CPU Intel Xeon Silver 4214 (2.2 GHz). Equipo proporcionado por el Laboratorio de Supercómputo del Bajío (Lab-SB) del CIMAT. 

\subsubsection{Evaluación de la Generación de Texto}

\textbf{Perplejidad.} La \emph{perplejidad} (PPL) es una métrica estándar para evaluar modelos de lenguaje, que mide qué tan bien un modelo predice una muestra. Matemáticamente, la perplejidad se define como la exponencial de la entropía cruzada promedio por token:
\[
\text{PPL} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_{<i})\right),
\]
donde \(N\) es el número total de tokens en el conjunto de evaluación, y \(P(w_i | w_{<i})\) es la probabilidad condicional del token \(w_i\) dado el contexto previo \(w_{<i}\). Una perplejidad más baja indica un mejor rendimiento del modelo, ya que sugiere que el modelo está menos "sorprendido" por los datos de evaluación.

Para evaluar el impacto del fine-tuning, se comparó la perplejidad del modelo ajustado con la del modelo base \texttt{mistral-7B-instruct-v0.3-bnb-4bit} sin ajuste fino. Esto permite cuantificar la mejora en la capacidad predictiva del modelo tras el entrenamiento con las letras de canciones. El código para calcular la perplejidad es proporcionado por la misma biblioteca \texttt{Unsloth}, se encuentra en \texttt{codigo/generative/mistral/utils/perplexity\_unsloth.py}. La lógica fue adaptada en el script \texttt{calculate\_perplexity.py} para evaluar ambos modelos (base y fine-tuned) sobre el conjunto de prueba.

El resultado de la evaluación de perplejidad en el conjunto de prueba es el siguiente:
\begin{itemize}
    \item Perplejidad del modelo base (sin fine-tuning): 9.35.
    \item Perplejidad del modelo fine-tuned: 7.20.
    \item Mejora relativa: \((9.35 - 7.20) / 9.35 \times 100\% \approx 22.93\%\).
    \item Interpretación: La reducción significativa en la perplejidad indica que el modelo fine-tuned ha mejorado su capacidad para predecir las letras de canciones, adaptándose mejor al estilo y vocabulario específico del dominio. Esto fue logrado a pesar del tamaño relativamente pequeño del conjunto de datos de entrenamiento, demostrando la eficacia del enfoque de fine-tuning con LoRA en un modelo cuantizado.
\end{itemize}

\textbf{Muestras generadas.} 

Se generaron varias muestras de texto utilizando el modelo fine-tuned para evaluar su capacidad de generación. Todas las muestras se generaron con los siguientes parámetros en la inferencia, por mostrar un mejor balance entre coherencia y diversidad:
\begin{itemize}
    \item \texttt{max\_new\_tokens = 256}. Representa la longitud máxima de la secuencia generada.
    \item \texttt{temperature = 0.8} (0.7 para Kendrick Lamar). Controla la aleatoriedad de la generación; valores más bajos hacen que el modelo sea más conservador.
    \item \texttt{top\_p = 0.9}. Utiliza muestreo de núcleo para limitar la selección de tokens a los más probables.
    \item \texttt{repetition\_penalty = 1.3}. Penaliza la repetición de tokens para fomentar la diversidad en la generación.
\end{itemize}

A continuación se presentan algunos ejemplos representativos:

\textbf{Ejemplo 1: Letra generada en el estilo de Jay-Z}

En este ejemplo, se le pidió al modelo que generara una letra en el estilo de Jay-Z mediante el prompt:

\begin{verbatim}
    You're a hip-hop artist, create song lyrics. 
    Write hip-hop lyrics in the style of Jay-Z.
\end{verbatim}

Y se obtuvo la siguiente salida:

\begin{verbatim}
    [Intro]
    Uh huh uh (yeah)
    Yeah yeah you know my name is Hov', right? Uhh
    I got this little thing called ``The Blueprint''
    And I just want to run through some shit for y'all tonight
    Just have fun with it
    (Hands up!)

    [Verse 1]
    Yo, yo, yo, yo, whattup! It's your boy Shawn
    Back on that bullshit 'cause niggas ain't learn
    Last time out I burned down the barn
    Now I'm back and I might burn down another one
    What do you wanna bet? You can put all ya chips in
    If he don't win then we all gonna be broke again
    But if he does win then we gon' get richer than rich men
    So place your bets now before they stop taking them
    See me as an investment like Berkley or Microsoft
    Put ten dollars into me baby watch how far it goes
    Watch how fast it grows
\end{verbatim}

Podemos analizar la letra generada entre pilares del estilo de cada artista:

\begin{itemize}
    \item \textbf{Persona y Autenticidad.} El intro es muy representativo del estilo de Jay-Z, utilizando \textit{ad-libs} (Uh, uh, yeah) y directamente referenciando su nickname (Hov'). También hace referencia a su álbum ``The Blueprint'', lo que refuerza su identidad como artista. El uso de ``Your boy Shawn'' es también auténtico.
    
    \item \textbf{Temas y Contenido.} La letra realiza una metáfora sobre invertir en sí mismo como un artista, lo cual es un tema recurrente en la música de Jay-Z.

    \item \textbf{Rimas y Fluidez.} La letra presenta un esquema de rima consistente (AABB), aunque la estructura es predecible y carece de complejidad y expertiz de Jay-Z.
\end{itemize}

\textbf{Ejemplo 2: Letra generada en el estilo de Kanye West}

Nuevamente, se le pidió al modelo la generación de una letra, ahora en el estilo de Kanye West. Esto fue mediante el prompt:

\begin{verbatim}
    You're a hip-hop artist, create song lyrics. 
    Write hip-hop lyrics in the style of Kanye West.
\end{verbatim}


Y se obtuvo la siguiente salida:

\begin{verbatim}
    [Intro]
    I know He won’t forsake me
    He won’t leave me
    No matter what I do
    And even if I go through hell tonight
    He will still be God when it’s all over with
    (God is) (God is)

    [Verse 1]
    They say build your own
    I said fuck that, we gon' buy our way out
    We ain't finna wait 'til they give us something to celebrate
    So let's get this bread
    Fuck up school and just drop out
    But you can't drop out without dropping some knowledge first
    Let me teach y'all how to make yourself rich
    How to stay fly while staying righteous
    First thing you need is faith as big as Mount Rushmore
    Then take it from here

    [Chorus]
    Jesus saved my soul
    Now I drive Rollies and Bentleys
    Bought myself a couple chains
    Spendin' money like I don't owe Him nothin'
    Everybody want heaven but nobody wanna die
    Nobody wanna cry no more
    That means everybody wanna live forever
    Not ready to meet Your maker
\end{verbatim}

\begin{itemize}
    \item \textbf{Persona y Autenticidad.} La letra captura la dualidad central de la persona de Kanye West: la del predicador y el materialista. El intro con temática góspel y las referencias a Dios son muy representativas de su era en los finales de los 10's con el album \textit{Jesus is King}. Esto se contrasta inmediatamente con un tono provocador y egocéntrico (Let me teach y'all'', fuck that''), lo cual es sumamente auténtico a su estilo.

    \item \textbf{Temas y Contenido.} El modelo representa exitosamente la mezcla de temas de Kanye. Se abordan la religión y la fe (``Jesus saved my soul'', ``faith as big as Mount Rushmore'') en conflicto directo con el materialismo extremo y el lujo (``drive Rollies and Bentleys''). Incluye también aforismos y comentarios sociales simplificados, como la línea ``Everybody want heaven but nobody wanna die'', un recurso muy común en sus letras.

    \item \textbf{Rimas y Fluidez.} Kanye a menudo prioriza el mensaje y la emoción sobre la complejidad de la rima. La letra generada refleja esto: el esquema de rima es inconsistente y conversacional. Se apoya más en la cadencia y en la fuerza de las declaraciones que en patrones de rima complejos, lo que resulta en un flujo que se asemeja a un sermón o una declaración apasionada, muy fiel a su estilo de entrega.
\end{itemize}


\textbf{Ejemplo 3: Letra generada en el estilo de Kendrick Lamar}

Esta vez se le pidió al modelo generar una letra para una canción de Hip-Hop al estilo de Kendrick Lamar, mediante el prompt:
    
\begin{verbatim}
    You're a hip-hop artist, create song lyrics. 
    Write hip-hop lyrics in the style of Kendrick Lamar.
\end{verbatim}

Y se obtuvo la siguiente salida:

\begin{verbatim}
    [Intro]
    I don’t know what they want from me! I gave them my life
    They wanna see some type of drama? Here it is
    (Bitch)

    [Verse 1]
    What up homie? What you writing about today? (Today?)
    Writing 'bout these niggas that be hating on me (Huh?)
    And all this fake love that surrounds me (Uh)
    All this bullshit that goes around me (Yeah)
    It gets tiring sometimes when you trying to stay positive
    But negative energy always surrounding you
    So how can we live with no stress and anxiety?
    When everybody got something bad to say bout you
    Everybody wants something but ain't willing to give nothin' back
    Nobody really cares if you win or lose at shit
    If your name isn't mentioned then nobody gon' mention you
    Even though you did more than most people ever done
    That'll make you feel like you worthless
    Like you never won
    Never been number one
    Always second place even when you first
    How do you deal with that kind of pressure? How do you cope?
    Knowin' every move you make will affect your whole family tree
    Your friends, your enemies, your lovers, haters, neighbors
    The world, society, everyone expect perfection
    From an imperfect person
\end{verbatim}
\begin{itemize}
    \item \textbf{Persona y Autenticidad.} Captura a la perfección la persona introspectiva, ansiosa y abrumada que define gran parte de la obra reciente de Kendrick Lamar (DAMN y MM\&BS). La letra se lee como un monólogo interno o una sesión de terapia, explorando la presión de la fama de una manera vulnerable y conflictiva, lo cual es una característica central de su estilo.

    \item \textbf{Temas y Contenido.} Los temas abordados son extremadamente fieles al universo lírico de Kendrick. Se centra en la salud mental (``stress and anxiety''), el peso de las expectativas (``everyone expect perfection''), la falsedad en la industria (``fake love that surrounds me''), y la responsabilidad generacional (``every move you make will affect your whole family tree''). Este enfoque en el conflicto interno y la carga del liderazgo es mucho más representativo que la simple bravuconería.

    \item \textbf{Rimas y Fluidez.} El modelo imita exitosamente la fluidez conversacional y casi de "spoken word" de Kendrick. En lugar de adherirse a un esquema de rima estricto, prioriza el flujo de conciencia y la carga emocional del mensaje. Las líneas son largas y discursivas, construyendo una tensión que culmina en la lista final (``Your friends, your enemies...''). Esta estructura, que favorece el ritmo narrativo sobre la rima perfecta, es una técnica clave en el repertorio de Kendrick.
\end{itemize}

En los tres ejemplos, el modelo fine-tuned demuestra una capacidad notable para capturar los estilos únicos de cada artista, tanto en términos de contenido temático como la estructura lírica. Aunque no alcanza la complejidad y profundidad de los artistas originales, especialmente en términos de rima y metáforas, el modelo genera letras coherentes y estilísticamente apropiadas para los artistas. Además, el modelo respetó en su mayoría la estructura de las letras basandose en los datos de entrenamiento (por ejemplo, siempre inicio con el [Intro], [Verse 1], etc.). Esto sugiere que el enfoque de fine-tuning con un conjunto de datos relativamente pequeño pero bien curado puede ser efectivo para adaptar un modelo de lenguaje grande a tareas creativas específicas como la generación de letras de canciones.

\newpage
\subsection{Modelos RNN/LSTM/GRU}

\subsubsection{RNN Simple}
Las RNN simples constituyen la forma canónica de modelar dependencias secuenciales mediante un estado oculto recurrente. Cada paso de tiempo aplica una transformación no lineal sobre la combinación del embedding del token actual y el estado oculto previo. Sin embargo, su profundidad temporal efectiva se ve limitada por el \emph{desvanecimiento} y la \emph{explosión} del gradiente, lo cual dificulta capturar dependencias de largo alcance en secuencias lingüísticas (estructuras repetitivas, coherencia temática, progresión narrativa, etc.).

	\textbf{Tokenización y representación.} Se entrenaron dos variantes: a nivel carácter y a nivel palabra. El vocabulario carácter contiene 157 símbolos (incluyendo saltos de línea, puntuación y caracteres Unicode presentes en las letras). El vocabulario palabra contiene 19{,}107 tokens (palabras, formas con apóstrofos y algunos préstamos multilingües). Cada token se proyecta a un embedding de dimensión 128.

	\textbf{Formato de datos.} El corpus en texto plano se segmenta en ventanas deslizantes de longitud fija \texttt{seq\_length = 100}. Para cada ventana de 100 tokens (o caracteres) se predice el siguiente token. No se mezcla información entre canciones más allá de la concatenación lineal del corpus, pero se preservan delimitadores estructurales (\texttt{<|song\_start|>} / \texttt{<song\_end>}), lo que provee señales débiles de segmentación.

	\textbf{Tamaño del conjunto.} El conjunto de entrenamiento contiene 820{,}993 caracteres (nivel de carácter) y 157{,}910 palabras (nivel de palabra). El conjunto de prueba contiene 214{,}957 caracteres y 41{,}889 palabras, respectivamente. El número de secuencias efectivas por nivel se calculan como (longitud$-$100). Con \texttt{batch\_size = 64}, esto produce aproximadamente 12.8K batches por época (carácter) y 2.5K batches por época (palabra), lo que impacta la relación señal–ruido de la estimación del gradiente (más estable en la variante palabra, más estocástica en la variante carácter).

	\textbf{Arquitectura.} La celda recurrente implementa la actualización clásica:
\[
h_t = \tanh(W_{xh} x_t + W_{hh} h_{t-1} + b),\quad y_t = W_{hy} h_t + c,
\]
donde $x_t \in \mathbb{R}^{128}$, $h_t \in \mathbb{R}^{256}$. Se emplean 2 capas apiladas (\texttt{num\_layers = 2}) con \texttt{dropout = 0.3} entre capas (no entre pasos temporales internos de cada capa).

\textbf{Capacidad y parámetros.} Aproximadamente:
\begin{itemize}
    \item Variante palabra: $\sim 7.59$M parámetros entrenables (embeddings $\approx$2.45M, núcleo recurrente $\approx$0.23M, proyección de salida $\approx$4.91M).
    \item Variante carácter: $\sim 0.29$M parámetros (la reducción proviene del vocabulario compacto).
\end{itemize}
Esto ilustra la fuerte asimetría de capacidad inducida por el tamaño del vocabulario y la capa de salida softmax.

	\textbf{Hiperparámetros de entrenamiento.} Se usó el optimizador Adam (\texttt{lr = 2e-3}), 20 épocas, \texttt{batch\_size = 64}, \texttt{seq\_length = 100}, \texttt{gradient clipping = 5.0}. La función de pérdida es la entropía cruzada. Un scheduler \texttt{ReduceLROnPlateau} reduce la tasa de aprendizaje con factor 0.5 tras 2 épocas sin mejora significativa (\texttt{patience = 2}). La pérdida se reporta en escala log-perplexity y puede re–expresarse como $\text{PPL} = e^{\mathcal{L}}$. Se habilitó opción de \emph{early stopping} configurable.

	\textbf{Regularización y estabilidad.} El \emph{dropout} inter–capas y el \emph{gradient clipping} controlan, respectivamente, sobre–ajuste y explosión de gradientes. Aún así, las limitaciones estructurales (sin mecanismos de compuertas ni memoria explícita) reducen su capacidad para retener contexto semántico más allá de decenas de pasos.

	\textbf{Hardware utilizado.} El mismo entorno descrito en la subsección de Transformers: 2× NVIDIA RTX TITAN (24 GB VRAM), 128 GB RAM y 24 núcleos CPU Intel Xeon Silver 4214, utilizando \texttt{PyTorch} para la implementación y entrenamiento.



\subsubsection{LSTM}
Las LSTM introducen compuertas que mitigan el desvanecimiento del gradiente y permiten retener información relevante a través de pasos temporales largos. La celda mantiene un estado de memoria $c_t$ separado del estado oculto $h_t$, modulando flujos de información mediante puertas sigmoides.

	\textbf{Tokenización y representación.} Se emplean las mismas dos granularidades (carácter y palabra) con embeddings de dimensión 128 inicializados aleatoriamente y aprendidos de forma conjunta. No se reutilizó el tokenizador sub‐palabras del modelo Transformer dado que aquí el foco es analizar arquitecturas recurrentes “clásicas”.

	\textbf{Formato de datos.} Idéntico al de la RNN simple: ventanas deslizantes de longitud 100, objetivo el token siguiente. La preservación de delimitadores de canción actúa como débil frontera semántica, y se espera que la LSTM capture patrones de estructura (introducciones, coros) con mayor robustez que la RNN simple.

	\textbf{Tamaño del conjunto.} Igual a la variante anterior; esto permite comparaciones controladas de arquitectura.

	\textbf{Arquitectura.} Dos capas LSTM (\texttt{hidden\_dim = 256}, \texttt{num\_layers = 2}) con \texttt{dropout = 0.3} entre capas. Ecuaciones de la celda (omitiendo subíndices de capa):
\[
\begin{aligned}
i_t &= \sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i),\\
f_t &= \sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f),\\
g_t &= \tanh(W_{xg} x_t + W_{hg} h_{t-1} + b_g),\\
o_t &= \sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o),\\
c_t &= f_t \odot c_{t-1} + i_t \odot g_t,\\
h_t &= o_t \odot \tanh(c_t).
\end{aligned}
\]

\textbf{Capacidad y parámetros.} Aprox.:
\begin{itemize}
    \item Variante palabra: $\sim 8.28$M parámetros (el incremento sobre RNN simple proviene del factor de 4 en matrices de compuertas).
    \item Variante carácter: $\sim 0.98$M parámetros.
\end{itemize}
El salto de capacidad frente a la RNN simple se concentra en las matrices de compuertas, incrementando la expresividad temporal.

\textbf{Hiperparámetros de entrenamiento.} Se mantuvieron los mismos valores globales para favorecer comparabilidad: \texttt{Adam}, 20 épocas de entrenamiento, \texttt{batch\_size = 64}, \texttt{seq\_length = 100}, \texttt{clip = 5.0}, scheduler adaptativo sobre la pérdida de validación y potencial \emph{early stopping}. La métrica primaria interna es la entropía cruzada; la perplexity derivada sirve para contrastes posteriores (no reportada aquí).

\textbf{Regularización y estabilidad.} El mecanismo de compuertas (en particular $f_t$) estabiliza el flujo de gradientes y atenúa el desvanecimiento típico. El \emph{dropout} entre capas actúa como regularizador estructural, y el \emph{gradient clipping} asegura control frente a ráfagas de magnitud.

\textbf{Ventajas observadas conceptualmente.} Mayor retención de contexto largo (estados temáticos, continuidad de narrador) y mejor manejo de secciones donde la progresión semántica se apoya en repeticiones moduladas (p. ej. estribillos). 


\subsubsection{GRU}
Las GRU (Gated Recurrent Units) simplifican la estructura de la LSTM fusionando algunas compuertas y eliminando el estado de memoria separado. Mantienen capacidad para mitigar el desvanecimiento del gradiente con menor costo parametrizable, lo que puede traducirse en entrenamientos más rápidos y menor riesgo de sobre–ajuste en conjuntos moderados.

	\textbf{Tokenización y representación.} Se reutiliza idéntica configuración (carácter y palabra, embeddings de 128). La comparación GRU vs LSTM bajo el mismo espacio de embeddings permite aislar el impacto de la diferencia arquitectónica.

	\textbf{Formato de datos.} Igual que en las dos arquitecturas previas (ventanas de 100 tokens → predicción del siguiente). Esto garantiza que cualquier diferencia empírica futura (perplejidad, estabilidad) se atribuya al diseño de la celda y no a variaciones de entrada.

	\textbf{Arquitectura.} Dos capas GRU de 256 unidades con \texttt{dropout = 0.3} entre capas. Las ecuaciones de la celda:
\[
\begin{aligned}
z_t &= \sigma(W_{xz} x_t + W_{hz} h_{t-1} + b_z) & \text{(puerta de actualización)}\\
r_t &= \sigma(W_{xr} x_t + W_{hr} h_{t-1} + b_r) & \text{(puerta de reinicio)}\\
    ilde{h}_t &= \tanh(W_{xh} x_t + W_{hh} (r_t \odot h_{t-1}) + b_h) & \text{(candidato)}\\
h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t.
\end{aligned}
\]

\textbf{Capacidad y parámetros.} Aproximadamente:
\begin{itemize}
    \item Variante palabra: $\sim 8.05$M parámetros (intermedio entre RNN simple y LSTM; 3 grupos de matrices en lugar de 1 o 4).
    \item Variante carácter: $\sim 0.75$M parámetros.
\end{itemize}
La reducción frente a LSTM puede favorecer menor sobre–ajuste en dominios con variabilidad estilística pero tamaño moderado.

\textbf{Hiperparámetros de entrenamiento.} Se conservaron los mismos (\texttt{Adam}, \texttt{lr = 2e-3}, \texttt{epochs = 20}, \texttt{batch\_size = 64}, \texttt{seq\_length = 100}, clipping 5.0, scheduler adaptativo). Esta homogeneidad facilita comparaciones directas posteriores de eficiencia y convergencia.

\textbf{Regularización y estabilidad.} El diseño de la puerta de actualización $z_t$ actúa como interpolador adaptativo entre memoria previa y nuevo contenido, reduciendo la necesidad de un estado separado tipo $c_t$. El \emph{dropout} inter–capas y el clipping complementan el control de generalización y estabilidad numérica.

\textbf{Consideraciones comparativas.} Conceptualmente, la GRU ofrece un compromiso: menos parámetros que LSTM, más expresión que la RNN simple. En escenarios con ventana de contexto moderada (100 tokens) y un dominio con repeticiones estilísticas claras (estructuras líricas), puede captar patrones de transición (Intro → Verso, Verso → Chorus) sin incurrir en el costo completo de las LSTM.


\subsubsection{Perplejidad de los Modelos Recurrentes}
La evaluación de perplejidad para RNN, LSTM y GRU se realizó mediante un esquema \emph{streaming} continuo: el conjunto de prueba completo se recorre secuencialmente en bloques de tamaño fijo (\texttt{block\_size = 100}), preservando el estado oculto entre bloques y calculando la pérdida en cada desplazamiento (teacher forcing con desfase 1). Este enfoque evita dos sesgos frecuentes: (i) usar solo la última posición de cada ventana y (ii) sub-muestrear tokens con ventanas disjuntas. No se reinició el estado en delimitadores de canción, lo cual puede reducir ligeramente la pérdida al permitir continuidad artificial; la decisión se mantuvo consistente entre arquitecturas y niveles.

\textbf{Resultados.}
Los valores agregados de PPL obtenidos mediante el esquema streaming se resumen en la \Cref{tab:perplex_rnn}. Cada cifra corresponde a la evaluación completa sobre el conjunto de prueba, preservando estado oculto entre bloques.

\begin{table}[H]
    \centering
    \begin{tabular}{l|cc}
        \hline
        Modelo & PPL (carácter) & PPL (palabra) \\
        \hline
        RNN  & 6.83 & 2{,}283.42 \\
        LSTM & \textbf{4.61} & \textbf{1{,}046.79} \\
        GRU  & 6.90 & 8{,}970.11 \\
        \hline
    \end{tabular}
    \caption{PPL de modelos recurrentes a nivel carácter y palabra.}
    \label{tab:perplex_rnn}
\end{table}

\textbf{Granularidad y escala.} Las perplejidades a nivel carácter son órdenes de magnitud menores que a nivel palabra por razones estructurales: (a) el vocabulario carácter (157 símbolos) induce un espacio de decisión logarítmicamente mucho más pequeño que el vocabulario palabra (19{,}107 tokens); (b) la distribución de frecuencias a nivel palabra sigue una cola larga (Zipf) que incrementa la entropía empírica y dificulta estimar probabilidades fiables con un corpus moderado; (c) la perplejidad no es comparable entre niveles, pues una palabra concentra la información de varios caracteres ($\approx$4–5 en promedio).

\textbf{Comparación arquitecturas (carácter).} Como se aprecia en la \Cref{tab:perplex_rnn}, LSTM obtiene la menor perplejidad (4.61) seguido de RNN (6.83) y GRU (6.90). La ventaja de LSTM concuerda con su mayor capacidad para retener dependencias temporales. El valor de GRU probablemente pueda descender con más épocas.

\textbf{Comparación arquitecturas (palabra).} La LSTM también lidera a nivel palabra (1{,}046.79), seguida de la RNN (2{,}283.42). La GRU presenta una perplejidad muy elevada (8{,}970.11), cercana al orden de magnitud del vocabulario, lo que sugiere sub-entrenamiento más que limitación intrínseca de la celda.


\textbf{Limitaciones.} La perplejidad (i) es sensible a la tokenización y no captura aspectos estilísticos (rima, coherencia, estructura lírica), (ii) puede estar ligeramente optimista al no reiniciar estados entre canciones, (iii) penaliza con fuerza tokens raros elevando cifras a nivel palabra y (iv) no mide repetición ni diversidad semántica. Se requiere complementarla con análisis cualitativo y métricas adicionales orientadas a estilo.

\subsubsection{Muestras generadas por Modelos Recurrentes}

En esta sección se presenta un análisis cualitativo enfocado en: (i) coherencia local y global, (ii) estructura lírica (uso de etiquetas y segmentos), (iii) repetición y degradación, (iv) rasgos estilísticos (temas, rimas aproximadas), y (v) diferencias atribuibles al nivel de tokenización. Se omite mostrar la salida completa por razones de espacio, pero se incluyen fragmentos representativos y observaciones clave.

\textbf{LSTM}

\begin{itemize}
    \item \textbf{Carácter:} Muestra fragmentos con continuidad sintáctica moderada: \textit{``I want you to the ... I said I was ... I think I was the days...''}. Aunque hay inserciones redundantes (``the''), consigue mantener sujeto implícito y verbos en secuencia plausible. La granularidad carácter permite interpolar transiciones suaves y ocasionales coincidencias fonéticas finales (pseudo-rimas débiles en terminaciones ``-ing'' / ``-ion'' cuando se generan). No se observa un bucle corto estable hasta después de varias oraciones truncadas, lo que sugiere dinámica de estado más rica.
    \item \textbf{Palabra:} Presenta progresión temática inicial relacionada con identidad y estatus (``the world'', ``game'', ``life''), pero deriva hacia repetición de construcciones \textit{``I got a ...''} y secuencias con alta frecuencia de un mismo sustantivo o pronombre. La presencia reiterada de marcadores de persona (\textit{I, you, nigga}) refleja patrones estadísticos del corpus. La segmentación por palabras acelera el colapso semántico: al errar una elección, el modelo tiende a reciclar tokens de alta probabilidad manteniendo superficialmente el tema pero sin avanzar narrativa. La perplejidad todavía relativamente alta frente a carácter se manifiesta como ciclos largos de repetición semántica.
\end{itemize}

\textbf{RNN}

\begin{itemize}
    \item \textbf{Carácter:} Se observan micro-bucles fonotácticos: \textit{``the see the see the''}, \textit{``the shit the''}, combinaciones que emergen por alta probabilidad condicional tras bigramas frecuentes. La ausencia de compuertas provoca degradación más rápida: el estado oculto acumula ruido y se reduce la diversidad contextual. Aun así, aparecen estructuras superficiales reconocibles (uso correcto de mayúscula inicial tras salto de línea, conservación de etiquetas como \texttt{[Verse 1: ...]} heredadas del prompt inicial).
    \item \textbf{Palabra:} El modelo genera un efecto de \textit{``eco''} semántico: repite \textit{``the whole industry''}, \textit{``I can be alright''}, \textit{``I know you don't''}. Estas repeticiones medianas (no sólo trigramas exactos repetidos inmediatamente) indican que el estado retiene fragmentos de alta frecuencia pero sin mecanismo para \emph{olvidar} selectivamente (a diferencia de LSTM/GRU). La coherencia discursiva cae cuando intenta introducir nuevos tópicos (mezcla de ``industry'', ``price'', ``sky'').
\end{itemize}


\textbf{GRU}

\begin{itemize}
    \item \textbf{Carácter:} Aunque la teoría sugiere mejor retención que RNN simple, en la muestra particular la calidad es similar o ligeramente inferior: secuencias como \textit{``the can the have my man in the hall''} muestran inserción de determinantes y auxiliares sin anclaje semántico. Posible causa: menor convergencia (véase su perplejidad más alta que LSTM en ambos niveles) y sensibilidad a inicialización al compartir hiperparámetros con menos épocas efectivas útiles.
    \item \textbf{Palabra:} Exhibe colapso severo temprano: cadenas extremas de \textit{``I got the world''}, \textit{``I got the shit''}, repeticiones densas de un mismo pronombre y sustantivo sin variación sintáctica. El valor de perplejidad cercano al tamaño del vocabulario sugiere predicciones casi uniformes para muchos contextos, lo que facilita que el muestreador (greedy o top-k reducido) se quede en órbitas de alta frecuencia y baja entropía local. El bucle temático centrado en autoafirmación y posesión es consistente con sesgos del corpus, pero la saturación semántica indica falta de calibración probabilística.
\end{itemize}

\textbf{Estructura y etiquetas.} Ninguna variante introduce de forma consistente las diferentes secciones de las canciones, reflejando que el modelo no internalizó reglas de transición macro–estructural, sino patrones locales.

\textbf{Rimas y sonoridad.} No se detectan rimas planeadas; las coincidencias de terminaciones similares en carácter provienen de repeticiones estadísticas. El entrenamiento limitado y la ausencia de un objetivo explícito de rima (ni condicionamiento métrico) restringen la emergencia de patrones poéticos formales.

Concluimos que a nivel carácter la LSTM produce la secuencia más fluida y con menos artefactos, seguida de RNN y luego GRU. A nivel palabra, todas las arquitecturas muestran \/``looping'' de motivos semánticos, pero la LSTM conserva mejor variación superficial antes de colapsar; la GRU exhibe un colapso temprano consistente con su perplejidad anómala. La RNN simple mantiene frases cortas relativamente legibles, aunque con pérdida rápida de progresión temática.

\subsection{Comparación entre Modelos Transformer y Recurrentes}

La comparación entre los modelos Transformer (Mistral) y las arquitecturas recurrentes (RNN, LSTM, GRU) revela diferencias significativas en varios aspectos clave:

\textbf{Perplejidad.} El modelo Transformer fine-tuned alcanzó una perplejidad de 7.20 en el conjunto de prueba, mientras que en la mejor puntuación de los modelos recurrentes (LSTM a nivel carácter) obtuvo 4.61 y la peor (GRU a nivel palabra) alcanzó 8,970.11. En la sección anterior se discutió que las perplejidades a nivel carácter y palabra no son directamente comparables debido a las diferencias en el tamaño del vocabulario y la naturaleza de la tokenización. Sin embargo, el Transformer aún con su vocabulario más grande (32,768) logró una perplejidad competitiva, lo que indica su capacidad para modelar dependencias a largo plazo y capturar patrones complejos en el texto.

\textbf{Coherencia y Fluidez.} En términos de coherencia local y global, el Transformer mostró una capacidad superior en todos los aspectos definidos en la evaluación. Logrando mantener temas y estructuras a lo largo de secuencias más largas. Las muestras generadas por el modelo Transformer presentaron transiciones más suaves entre ideas y una mejor progresión narrativa en comparación con las arquitecturas recurrentes, que tendían a colapsar en bucles repetitivos o perder coherencia después de varias oraciones.

En concreto, las RNNs solían empeorar rápidamente con la longitud de la secuencia, mientras que el Transformer mantenía la coherencia incluso en textos más extensos. La LSTM fue la arquitectura recurrente que mejor manejó la coherencia, pero aún así no alcanzó el nivel del Transformer. Incluso considerando valores más altos de temperatura, las redes tipo RNN no lograron igualar la fluidez y diversidad del Transformer (y hasta era contraproducente, este efecto fue muy fuerte en los modelos a nivel de letra).




\newpage
\section{Parte B: Clasificación de Texto}

\subsection{Datos}

Los datos utilizados para la tarea de clasificación de texto son una muestra proveniente de la base de datos \textit{REST-MEX 2025}, un conjunto de reseñas de negocios en los pueblos mágicos de México. Este conjunto de datos contiene reseñas de usuarios en formato de texto, junto con una etiqueta que indica la calificación del negocio en una escala de 1 a 5 estrellas.

\subsubsection{Limpieza y Preprocesamiento de Datos}

El archivo de entrada (\texttt{meia\_data.csv}) contiene las columnas: \texttt{Review} (texto de la reseña), \texttt{Polarity} (calificación), \texttt{Town}, \texttt{Region} y \texttt{Type}. Sin embargo, en este proyecto solo se utilizaron las columnas \texttt{Review} y \texttt{Polarity}. El preprocesamiento de los datos incluyó los siguientes pasos:

\textbf{Corrección de codificación.} El conjunto presenta problemas de mojibake (caracteres mal codificados) heredados de conversiones entre encodings. Se aplica una corrección mediante re-codificación: cada texto se codifica a \texttt{latin-1} y se decodifica a \texttt{utf-8} con \texttt{errors='ignore'}, recuperando la mayoría de caracteres especiales del español (tildes, eñes, etc.). Esta transformación se implementa en la función \texttt{fix\_mojibake}.

\textbf{Selección de características.} Se descartan las columnas \texttt{Town}, \texttt{Region} y \texttt{Type}, conservando únicamente el texto de la reseña y su polaridad. Esto se justifica al priorizar la señal lingüística pura sobre metadatos geográficos o categóricos, alineándose con el enfoque de modelado puramente textual.

\textbf{Normalización de etiquetas.} Las etiquetas de polaridad originales (rango 1–5) se transforman a índices 0-indexados (rango 0–4) mediante la operación $\text{label} = \text{Polarity} - 1$. Esta conversión es estándar en PyTorch, donde las clases deben comenzar desde 0 para compatibilidad con \texttt{nn.CrossEntropyLoss}.

\textbf{Particiones del conjunto.} Se aplicó una división estratificada mediante \texttt{train\_test\_split} de scikit-learn con \texttt{random\_state=42} para reproducibilidad:
\begin{itemize}
    \item \textbf{Entrenamiento}: 72\% del total (primera división 80\% train, luego 90\% de ese subconjunto).
    \item \textbf{Validación}: 8\% del total (10\% del subconjunto de entrenamiento).
    \item \textbf{Prueba}: 20\% del total.
\end{itemize}
La estratificación garantiza que la distribución de clases se preserve en cada partición, evitando sesgos por desbalance de polaridad.

\textbf{Construcción de vocabulario.} Para los modelos RNN/LSTM/GRU y CNN se construyó un vocabulario a partir del conjunto de entrenamiento exclusivamente, previniendo filtración de información (data leakage). El proceso implementado en \texttt{build\_vocab} incluye:
\begin{enumerate}
    \item \textbf{Tokenización simple}: Se emplea una expresión regular \texttt{\textbackslash b\textbackslash w+\textbackslash b} sobre texto en minúsculas, extrayendo secuencias alfanuméricas y descartando puntuación.
    \item \textbf{Conteo de frecuencias}: Se acumulan las ocurrencias de cada token mediante \texttt{Counter}.
    \item \textbf{Filtrado por frecuencia mínima}: Se retienen únicamente tokens con frecuencia $\geq$ \texttt{min\_freq} (valor por defecto: 2). Este umbral mitiga ruido de errores tipográficos y hapax legomena.
    \item \textbf{Limitación de tamaño}: Opcionalmente se puede restringir el vocabulario a los \texttt{max\_vocab\_size} tokens más frecuentes (por defecto: ilimitado).
    \item \textbf{Tokens especiales}: Se reservan índices fijos para:
    \begin{itemize}
        \item \texttt{<PAD>} (índice 0): Padding de secuencias.
        \item \texttt{<UNK>} (índice 1): Tokens fuera de vocabulario.
    \end{itemize}
\end{enumerate}

En la ejecución con parámetros predeterminados (\texttt{min\_freq=2}, sin límite de tamaño), el vocabulario final contiene aproximadamente 12,000–15,000 tokens (incluyendo especiales), dependiendo del corpus específico.

\textbf{Estadísticas de longitud de secuencia.} Se calculan métricas sobre las longitudes tokenizadas del conjunto de entrenamiento para informar la elección del hiperparámetro \texttt{max\_seq\_length}:
\begin{itemize}
    \item Mínimo, máximo, media y mediana.
    \item Percentiles 95 y 99: recomendados como valores de corte para padding/truncamiento, balanceando cobertura de información y eficiencia computacional.
\end{itemize}
Estas estadísticas se almacenan en \texttt{meia\_data\_stats.json} para consulta posterior.

\textbf{Formatos de salida.} El script genera archivos diferenciados según el tipo de modelo:

\textit{Para modelos Transformer (mDeBERTa):}
\begin{itemize}
    \item \texttt{meia\_data.json}: Archivo JSON estructurado con tres claves (\texttt{train}, \texttt{validation}, \texttt{test}), cada una conteniendo una lista de diccionarios con campos \texttt{text}, \texttt{label} y \texttt{uuid}. Este formato es compatible con \texttt{datasets.load\_dataset} de Hugging Face.
\end{itemize}

\textit{Para modelos RNN/LSTM/GRU/CNN:}
\begin{itemize}
    \item \texttt{meia\_data\_train.csv}, \texttt{meia\_data\_val.csv}, \texttt{meia\_data\_test.csv}: Archivos CSV con columnas \texttt{text}, \texttt{label}, \texttt{uuid}. Formato directo para \texttt{pandas.read\_csv}.
    \item \texttt{meia\_data\_vocab.json}: Diccionario JSON con mapeo palabra$\to$índice. Incluye tokens especiales y todos los términos filtrados por frecuencia mínima.
    \item \texttt{meia\_data\_stats.json}: Diccionario JSON con estadísticas de longitud (\texttt{min}, \texttt{max}, \texttt{mean}, \texttt{median}, \texttt{percentile\_95}, \texttt{percentile\_99}).
\end{itemize}

\textbf{Ejecución del script.} El preprocesamiento se invoca desde terminal mediante:
\begin{center}
\texttt{python ./codigo/classification/prepare\_data.py \textbackslash} \\
\texttt{~~--file\_path "./data/classification/meia\_data.csv" \textbackslash} \\
\texttt{~~--output\_path "./data/classification/meia\_data.json"}
\end{center}

Con parámetros predeterminados, el proceso completa en segundos y reporta:
\begin{itemize}
    \item Tamaños de las particiones (e.g., Train: 18,003, Val: 2,001, Test: 5,001).
    \item Vocabulario final con $\sim$12,349 tokens (ejemplo representativo).
    \item Estadísticas de longitud (e.g., percentil 95: 98 tokens, percentil 99: 156 tokens).
\end{itemize}

La separación entre formatos responde a las diferencias arquitectónicas: los Transformers emplean tokenizadores sub-palabra preentrenados (SentencePiece, WordPiece) que operan internamente, mientras que las RNN/CNN requieren vocabularios discretos explícitos y padding manual. La construcción del vocabulario exclusivamente sobre entrenamiento evita sesgo optimista en evaluación, y el filtrado por frecuencia mínima equilibra cobertura léxica con complejidad parametrizable (dimensión del embedding y capa softmax).

\newpage

\subsection{Transformers}

\subsubsection{mDeBERTa V3}

\textbf{DeBERTa} (Decoding-enhanced BERT with disentangled attention) es una arquitectura de modelo de lenguaje basada en Transformers que introduce varias mejoras sobre el modelo BERT original. Las principales características de DeBERTa incluyen:
\begin{itemize}
    \item \textbf{Atención Desentrelazada (Disentangled Attention):} A diferencia de la atención estándar que combina las representaciones de posición y contenido, DeBERTa las trata por separado. Esto permite que el modelo capture mejor las relaciones entre palabras en función de su posición relativa y su contenido semántico.
    \item \textbf{Posiciones Absolutas y Relativas:} DeBERTa utiliza una combinación de codificaciones de posición absolutas y relativas, lo que mejora la capacidad del modelo para entender el contexto de las palabras en una oración.
    \item \textbf{Capa de Decodificación Mejorada:} La arquitectura de DeBERTa incluye una capa de decodificación que mejora la capacidad del modelo para generar representaciones más ricas y contextualmente relevantes.
\end{itemize}

En este proyecto se utilizó la variante \textbf{mDeBERTa V3}, que es una versión multilingüe de DeBERTa, con 0.3B de parámetros y fue preentrenada en múltiples idiomas, incluyendo el español. Esto la hace especialmente adecuada para tareas de procesamiento de lenguaje natural en contextos multilingües, y por tanto, en el nuestro, para clasificar reseñas en español.

\subsubsection{Configuración del Fine-Tuning}

El fine-tuning del modelo mDeBERTa V3 se realizó utilizando la biblioteca \texttt{transformers} de Hugging Face. En esta ocasión se decidió utilizar un enfoque de fine-tuning completo, ajustando todos los parámetros del modelo preentrenado debido a su reducido tamaño en comparación a Mistral 7B. A continuación se detallan los aspectos clave de la configuración del fine-tuning:

\textbf{Tokenizador y representación.} El modelo emplea el tokenizador de DeBERTa basado en SentencePiece con un vocabulario de aproximadamente 128K tokens. Este tokenizador multilingüe fue preentrenado en 100 idiomas, incluyendo español, lo que garantiza cobertura adecuada para el dominio de reseñas turísticas sin necesidad de ajuste léxico adicional. La tokenización sub-palabra permite manejar eficientemente variaciones morfológicas del español (género, número, conjugaciones) y errores ortográficos frecuentes en texto generado por usuarios.

\textbf{Formato de datos.} Cada ejemplo se estructura como un par texto–etiqueta, donde el texto corresponde a la reseña completa y la etiqueta es la polaridad 0-indexada (rango 0–4, correspondiente a 1–5 estrellas originales). Los datos se cargan desde archivos JSON con estructura \texttt{\{train, validation, test\}} mediante \texttt{datasets.Dataset.from\_list}. La tokenización se aplica mediante \texttt{tokenizer()} con truncamiento a \texttt{max\_seq\_length} y sin padding inicial; el padding dinámico se gestiona posteriormente con \texttt{DataCollatorWithPadding} para optimizar eficiencia por lote.

\textbf{Tamaño del conjunto.} El corpus preprocesado contiene 3600 ejemplos de entrenamiento, 400 de validación y 1000 de prueba. Con un tamaño de lote efectivo de 32 secuencias (lote por dispositivo = 8, acumulación de gradientes = 4 y 2 GPUs), el número de pasos por época es:
\[
\text{steps/epoch} = \lceil 3600 / (32\times2) \rceil = 57.
\]
Se entrenaron 3 épocas con parámetros por defecto, resultando en aproximadamente 171 pasos totales. Se programaron evaluaciones cada 100 pasos (\texttt{eval\_steps = 100}) y se almacenaron checkpoints con la misma frecuencia, conservando únicamente los 3 mejores modelos según F1 en validación.

\textbf{Longitud de secuencia.} Se fijó \texttt{max\_seq\_length = 512} tokens, valor que cubre el percentil 99 de longitudes del conjunto (156 tokens tras tokenización). Las secuencias más largas se truncan y las cortas se completan con padding dinámico por lote, minimizando cómputo desperdiciado en tokens especiales.

\textbf{Arquitectura fine-tuned.} Se ajustaron todos los parámetros del modelo base (12 capas transformer, \texttt{hidden\_size = 768}, 12 cabezas de atención), añadiendo únicamente una capa de clasificación lineal (\texttt{nn.Linear(768, 5)}) sobre el token \texttt{[CLS]} para proyectar representaciones a logits de 5 clases. El modelo completo contiene aproximadamente 280M parámetros entrenables. No se aplicó cuantización ni técnicas de eficiencia parametrizable (LoRA/adaptadores), optando por fine-tuning completo debido al tamaño relativamente manejable del modelo y la disponibilidad de hardware.

\textbf{Hiperparámetros de entrenamiento.} Se utilizaron los valores por defecto del script: tasa de aprendizaje \texttt{2e-5} (típica para fine-tuning de Transformers), scheduler lineal con un warmup ratio de 0.1 (primer 10\% de pasos con calentamiento), 3 épocas de entrenamiento, un tamaño de lote por dispositivo de 8, 4 pasos de acumulación de gradiente, \texttt{weight\_decay = 0.01}, optimizador \texttt{AdamW} estándar, y semilla \texttt{42}. El tamaño de lote efectivo resultante es 32 ejemplos. Se habilitó precisión mixta FP16 (\texttt{fp16 = True}) cuando GPU está disponible, reduciendo uso de memoria y acelerando cómputo sin pérdida significativa de precisión.

\textbf{Estrategia de evaluación y selección.} Se implementó early stopping implícito mediante \texttt{load\_best\_model\_at\_end = True} con métrica objetivo \texttt{f1} (weighted). Tras cada evaluación periódica, el modelo se compara con el mejor checkpoint previo y se conserva únicamente si supera el F1 de validación. Esta estrategia mitiga sobre-ajuste en conjuntos pequeños y garantiza que el modelo final corresponde al punto de máxima generalización observada.

\textbf{Métricas de evaluación.} Se calculan accuracy, precision, recall y F1-score (weighted) mediante \texttt{sklearn.metrics}, además de métricas por clase individual. La matriz de confusión completa se registra al finalizar el entrenamiento para análisis detallado de errores sistemáticos entre clases adyacentes (p.ej. confusión 3-4 estrellas).

\textbf{Gestión de memoria y hardware.} Con fine-tuning completo de 280M parámetros en FP16, el pico de VRAM por dispositivo es aproximadamente 4–6 GB (incluyendo activaciones, gradientes y estado del optimizador), lo cual cabe cómodamente en GPUs modernas de 8+ GB. El padding dinámico reduce desperdicio de memoria frente a padding estático global. El script registra memoria reservada y máxima por dispositivo antes y después del entrenamiento para monitoreo.

\textbf{Hardware utilizado.} El mismo entorno del fine-tuning de Mistral: 2× NVIDIA RTX TITAN (24 GB VRAM cada una), 128 GB RAM y 24 núcleos CPU Intel Xeon Silver 4214 (2.2 GHz), proporcionado por el Lab-SB del CIMAT. El entrenamiento completó en aproximadamente 45–60 minutos con el hardware disponible.


\subsection{Modelos RNN/LSTM/GRU}

Los modelos recurrentes (RNN, LSTM, GRU) comparten una arquitectura unificada implementada en la clase \texttt{RNNType}, que permite seleccionar el tipo de celda recurrente mediante el parámetro \texttt{rnn\_type}. Esta abstracción facilita la experimentación comparativa manteniendo idéntica la infraestructura de embedding, dropout y clasificación.

\textbf{Arquitectura Común}

Todos los modelos recurrentes siguen la misma estructura de capas:
\begin{enumerate}
    \item \textbf{Capa de Embedding}: Proyecta índices discretos de tokens (rango 0–\textit{vocab\_size}) a vectores densos de dimensión \texttt{embedding\_dim}. Se configura con \texttt{padding\_idx=0} para que los tokens de padding (\texttt{<PAD>}) no contribuyan al gradiente.
    
    \item \textbf{Capas Recurrentes}: Se apilan \texttt{num\_layers} capas del tipo seleccionado, procesando la secuencia de embeddings y propagando información temporal. La celda recurrente actualiza su estado oculto en cada paso temporal según la arquitectura específica.
    
    \item \textbf{Dropout}: Tras la última capa recurrente, se aplica dropout al último estado oculto para regularización.
    
    \item \textbf{Capa Completamente Conectada}: Una proyección lineal \texttt{nn.Linear(hidden\_dim, num\_classes)} transforma el último estado oculto en logits de clasificación para las 5 clases.
\end{enumerate}

El flujo de propagación es el siguiente:
\[
\text{Entrada} \xrightarrow{\text{Embed.}} \text{Seq. de vectores} \xrightarrow{\text{RNN/LSTM/GRU}} \text{Último estado oculto} \xrightarrow{\text{Dropout}} \xrightarrow{\text{FC}} \text{Logits}
\]

La función de pérdida empleada es \texttt{CrossEntropyLoss}, que combina softmax y negative log-likelihood, apropiada para clasificación multiclase. El optimizador seleccionado es \texttt{Adam} con tasa de aprendizaje predeterminada de \texttt{1e-3}.

\subsubsection{RNN}

\textbf{Arquitectura}

La RNN simple es la arquitectura recurrente más básica. Es propensa al problema de \textit{vanishing gradients} en secuencias largas, lo que limita su capacidad de capturar dependencias a largo plazo. Sin embargo, su simplicidad la hace computacionalmente eficiente, aunque menos paralelizable que otro tipos de redes.

\textbf{Configuración}

Los hiperparámetros predeterminados para el entrenamiento de RNN son:
\begin{itemize}
    \item \textbf{Dimensión de embedding}: \texttt{embedding\_dim = 128}
    \item \textbf{Dimensión oculta}: \texttt{hidden\_dim = 256}
    \item \textbf{Número de capas}: \texttt{num\_layers = 2}
    \item \textbf{Dropout}: \texttt{dropout = 0.3}
    \item \textbf{Longitud máxima de secuencia}: \texttt{max\_length = 128}
    \item \textbf{Tasa de aprendizaje}: \texttt{lr = 1e-3}
    \item \textbf{Épocas}: \texttt{epochs = 50}
    \item \textbf{Tamaño de lote}: \texttt{batch\_size = 32}
    \item \textbf{Paciencia (early stopping)}: \texttt{patience = 15}
\end{itemize}

Con vocabulario de $\sim$8033 tokens y 5 clases, el modelo RNN contiene aproximadamente:
\[
\text{Parámetros} \approx (8033 \times 128) + 2 \times [(128 \times 256) + (256 \times 256)] + (256 \times 5) \approx 1.26 \text{M parámetros}
\]

\subsubsection{LSTM}

\textbf{Arquitectura}

La LSTM introduce un mecanismo de compuertas para controlar el flujo de información y mitigar el problema de vanishing gradients. Incorpora un estado de celda $C_t$ además del estado oculto $h_t$.

\textbf{Configuración}

Los hiperparámetros predeterminados son idénticos a RNN:
\begin{itemize}
    \item \textbf{Dimensión de embedding}: \texttt{embedding\_dim = 128}
    \item \textbf{Dimensión oculta}: \texttt{hidden\_dim = 256}
    \item \textbf{Número de capas}: \texttt{num\_layers = 2}
    \item \textbf{Dropout}: \texttt{dropout = 0.3}
    \item \textbf{Longitud máxima de secuencia}: \texttt{max\_length = 128}
    \item \textbf{Tasa de aprendizaje}: \texttt{lr = 1e-3}
    \item \textbf{Épocas}: \texttt{epochs = 50}
    \item \textbf{Tamaño de lote}: \texttt{batch\_size = 32}
    \item \textbf{Paciencia (early stopping)}: \texttt{patience = 15}
\end{itemize}

Debido a las compuertas adicionales, LSTM tiene aproximadamente 4× más parámetros que RNN simple en las capas recurrentes:
\[
\text{Parámetros} \approx (8033 \times 128) + 2 \times [4 \times ((128 \times 256) + (256 \times 256))] + (256 \times 5) \approx 1.95 \text{M parámetros}
\]

\subsubsection{GRU}

\textbf{Arquitectura}

La GRU (\textit{Gated Recurrent Unit}) es una variante simplificada de LSTM que combina el estado de celda y el estado oculto en uno solo, reduciendo el número de compuertas a dos. GRU ofrece un balance entre la capacidad expresiva de LSTM y la eficiencia computacional de RNN simple, con menos parámetros que LSTM pero mejor manejo de dependencias largas que RNN.

\textbf{Configuración}

Los hiperparámetros predeterminados mantienen consistencia con RNN y LSTM:
\begin{itemize}
    \item \textbf{Dimensión de embedding}: \texttt{embedding\_dim = 128}
    \item \textbf{Dimensión oculta}: \texttt{hidden\_dim = 256}
    \item \textbf{Número de capas}: \texttt{num\_layers = 2}
    \item \textbf{Dropout}: \texttt{dropout = 0.3}
    \item \textbf{Longitud máxima de secuencia}: \texttt{max\_length = 128}
    \item \textbf{Tasa de aprendizaje}: \texttt{lr = 1e-3}
    \item \textbf{Épocas}: \texttt{epochs = 50}
    \item \textbf{Tamaño de lote}: \texttt{batch\_size = 32}
    \item \textbf{Paciencia (early stopping)}: \texttt{patience = 15}
\end{itemize}

GRU tiene aproximadamente 3× más parámetros que RNN en las capas recurrentes (3 transformaciones lineales vs. 1):
\[
\text{Parámetros} \approx (8033 \times 128) + 2 \times [3 \times ((128 \times 256) + (256 \times 256))] + (256 \times 5) \approx 1.72 \text{M parámetros}
\]

\textbf{Estrategia de entrenamiento unificada.} Los tres modelos se entrenan con el mismo protocolo: optimizador Adam, función de pérdida CrossEntropyLoss, métricas de evaluación (accuracy, precision, recall, F1 weighted y por clase, matriz de confusión), y early stopping basado en F1 de validación con paciencia de 15 épocas. El mejor modelo se selecciona según máximo F1 en validación, y se evalúa finalmente en el conjunto de prueba. Esta consistencia metodológica permite comparación justa entre arquitecturas recurrentes.

\newpage
\subsection{Modelo CNN}

\subsubsection{Arquitectura}

La arquitectura \textbf{TextCNN} implementada se basa en el trabajo seminal de Kim para clasificación de oraciones \cite{kim2014convolutional}. A diferencia de los modelos recurrentes que procesan secuencias de manera temporal, las CNNs capturan patrones locales (n-gramas) mediante convoluciones paralelas con múltiples tamaños de kernel.

\textbf{Estructura del modelo:}
\begin{enumerate}
    \item \textbf{Capa de Embedding}: Idéntica a los modelos recurrentes, proyecta tokens a vectores de dimensión \texttt{embedding\_dim = 128} con \texttt{padding\_idx = 0}.
    
    \item \textbf{Convoluciones Paralelas}: Se aplican $k$ filtros convolucionales 1D con diferentes tamaños de kernel $\{3, 4, 5\}$ (predeterminado) en paralelo sobre la secuencia de embeddings. Cada kernel captura diferentes n-gramas:
    \begin{itemize}
        \item Kernel size = 3: Captura trigramas (patrones de 3 palabras consecutivas)
        \item Kernel size = 4: Captura 4-gramas
        \item Kernel size = 5: Captura 5-gramas
    \end{itemize}
    Cada convolución genera \texttt{num\_filters = 100} mapas de características, seguidos de activación ReLU.
    
    \item \textbf{Max-Pooling sobre Tiempo}: Para cada mapa de características generado por las convoluciones, se aplica max-pooling global (\textit{max-over-time pooling}) que extrae el valor máximo a lo largo de toda la secuencia. Esta operación captura la característica más importante detectada por cada filtro, independientemente de su posición en el texto, proporcionando invarianza posicional.
    
    \item \textbf{Concatenación}: Los vectores resultantes del max-pooling de todos los kernels se concatenan en un único vector de características de dimensión $\text{len(kernel\_sizes)} \times \text{num\_filters} = 3 \times 100 = 300$.
    
    \item \textbf{Dropout}: Se aplica dropout con probabilidad \texttt{0.5} al vector concatenado para regularización.
    
    \item \textbf{Capa Completamente Conectada}: Proyección lineal final del vector de 300 dimensiones a logits de 5 clases.
\end{enumerate}

El flujo completo es:
\[
\text{Entrada} \xrightarrow{\text{Embedding}} \text{(B, L, E)} \xrightarrow{\text{Transpose}} \text{(B, E, L)} \xrightarrow{\text{Conv1d + ReLU}} \text{(B, F, L')}
\]
\[
\xrightarrow{\text{MaxPool}} \text{(B, F)} \xrightarrow{\text{Concat}} \text{(B, 3F)} \xrightarrow{\text{Dropout}} \xrightarrow{\text{FC}} \text{(B, C)}
\]
donde $B$ = batch size, $L$ = longitud de secuencia, $E$ = embedding dim, $F$ = num\_filters, $C$ = num\_classes.

Las CNNs son altamente paralelizables y eficientes computacionalmente, permitiendo entrenamiento rápido incluso en hardware modesto. Sin embargo, carecen de mecanismos explícitos para modelar dependencias a largo plazo, confiando en la combinación de múltiples filtros y pooling para capturar patrones relevantes.

\subsubsection{Configuración}

Los hiperparámetros predeterminados para el entrenamiento de TextCNN son:
\begin{itemize}
    \item \textbf{Dimensión de embedding}: \texttt{embedding\_dim = 128}
    \item \textbf{Número de filtros}: \texttt{num\_filters = 100} (por cada tamaño de kernel)
    \item \textbf{Tamaños de kernel}: \texttt{kernel\_sizes = [3, 4, 5]}
    \item \textbf{Dropout}: \texttt{dropout = 0.5}
    \item \textbf{Longitud máxima de secuencia}: \texttt{max\_length = 128}
    \item \textbf{Tasa de aprendizaje}: \texttt{lr = 1e-3}
    \item \textbf{Épocas}: \texttt{epochs = 50}
    \item \textbf{Tamaño de lote}: \texttt{batch\_size = 32}
    \item \textbf{Paciencia (early stopping)}: \texttt{patience = 10}
\end{itemize}

Con vocabulario de $\sim$8,033 tokens, la cuenta de parámetros es:
\[
\text{Parámetros} \approx (8,033 \times 128) + 3 \times (128 \times 100 \times k_i) + (300 \times 5) \approx 1.18 \text{M parámetros}
\]
donde $k_i \in \{3, 4, 5\}$ representa cada tamaño de kernel convolucional.

\textbf{Función de pérdida y optimización}: Al igual que los modelos recurrentes, se emplea \texttt{CrossEntropyLoss} y optimizador \texttt{Adam}. Las métricas de evaluación son idénticas (accuracy, precision, recall, F1 weighted y por clase, matriz de confusión), y el modelo se selecciona mediante early stopping basado en F1 de validación. El uso del mismo protocolo de evaluación garantiza comparabilidad directa entre arquitecturas convolutivas y recurrentes.

\newpage

\subsection{Comparación entre los Modelos}

\subsubsection{Métricas de clasificación}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figure/rnn-results.pdf}
        \caption{Simple RNN}
        \label{fig:rnn_cm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figure/gru-results.pdf}
        \caption{GRU}
        \label{fig:gru_cm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figure/lstm-results.pdf}
        \caption{LSTM}
        \label{fig:lstm_cm}
    \end{subfigure}

    \vspace{1em} % Adds a little vertical space between the rows


    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figure/cnn-results.pdf}
        \caption{CNN}
        \label{fig:cnn_cm}
    \end{subfigure}
    \hspace{1cm}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figure/mdeberta_v3-results.pdf}
        \caption{MDeBERTa v3}
        \label{fig:mdeberta_cm}
    \end{subfigure}

    \caption{Matrices de confusión de los modelos entrenados para clasificación de texto. Cada fila representa la clase verdadera y cada columna la clase predicha. El conjunto de prueba contiene 160 ejemplos para la clase 1, 180 para la clase 2, 200 para la clase 3, 220 para la clase 4 y 240 para la clase 5.}
    \label{fig:cms}
\end{figure}

La Figura~\ref{fig:cms} presenta las matrices de confusión para todos los modelos entrenados, permitiendo analizar patrones de error específicos por clase y arquitectura. Las matrices revelan diferencias significativas en el comportamiento de cada modelo frente a la tarea de clasificación.

En particular, la red RNN Simple (Figura~\ref{fig:rnn_cm}) presenta el peor desempeño con una tendencia marcada a predecir las clases 3 y 5, reflejando su incapacidad para capturar patrones discriminativos en el texto. La GRU (Figura~\ref{fig:gru_cm}) muestra mejora respecto a RNN pero mantiene sesgo hacia clases superiores (4 y 5). La LSTM (Figura~\ref{fig:lstm_cm}) alcanza el mejor desempeño entre las variantes recurrentes, con una matriz más equilibrada y diagonal pronunciada, se observa la confusión entre clases adyacentes y una disminución entre clases separadas. La CNN (Figura~\ref{fig:cnn_cm}) supera a LSTM, mostrando una reducción de la confusión entre clases no adyacentes. mDeBERTa-v3 (Figura~\ref{fig:mdeberta_cm}) obtiene el mejor rendimiento entre todos los modelos; elimina casi por completo el error entre las clases no adyacentes y presenta una diagonal muy definida, aunque persiste cierta confusión entre las clases 1 y 2.

\begin{table}[H]
\centering

\begin{tabular}{lccccc}
\toprule
\textbf{Métrica} & \textbf{RNN} & \textbf{GRU} & \textbf{LSTM} & \textbf{CNN} & \textbf{mDeBERTa-v3} \\
\midrule
Accuracy         & 0.229        & 0.281        & 0.413         & 0.445        & \textbf{0.547} \\
F1 Macro         & 0.126        & 0.281        & 0.412         & 0.443        & \textbf{0.529} \\
F1 Weighted      & 0.140        & 0.279        & 0.417         & 0.447        & \textbf{0.534} \\
\bottomrule
\end{tabular}
\caption{Métricas de evaluación en el conjunto de prueba para diferentes arquitecturas}
\label{tab:metricas_modelos}
\end{table}

La Tabla~\ref{tab:metricas_modelos} resume las métricas cuantitativas de rendimiento, revelando una progresión clara: RNN Simple sufre colapso representacional con discrepancia entre F1 macro y weighted indicando sesgo hacia clases específicas; GRU mejora modestamente con métricas F1 15balanceadas, gracias a compuertas pero mantiene rendimiento absoluto bajo; LSTM marca salto cualitativo (+18.4 puntos sobre RNN) con F1 consistentes validando eficacia de memoria a largo plazo; CNN supera todas las recurrentes (+3.2 sobre LSTM) demostrando superioridad de patrones n-gramáticos locales con menor complejidad parametrizable (1.18M vs 1.95M); mDeBERTa-v3 lidera con mejora relativa de 23\% sobre CNN y 100\% sobre RNN, atribuible a atención desentrelazada, preentrenamiento multilingüe en corpus masivo y 280M parámetros para matices semánticos finos. El salto más significativo ocurre entre modelos from-scratch ($\leq$44.5\%) y preentrenado (54.7\%), subrayando criticidad de transfer learning en conjuntos pequeños (3,600 ejemplos), aunque ningún modelo supera 55\% reflejando dificultad intrínseca de clasificación fine-grained en 5 clases con límites difusos entre categorías adyacentes.


\subsubsection{Requerimientos de Hardware y Tiempo de Entrenamiento}

\begin{table}[H]
\centering
\begin{tabular}{lrrrrr}
\toprule
\textbf{Métrica} & \textbf{RNN} & \textbf{GRU} & \textbf{LSTM} & \textbf{CNN} & \textbf{mDeBERTa-v3} \\
\midrule
Tiempo (min) & 0.42 & 3.12 & 3.09 & 0.57 & 14.71 \\
Max VRAM (GB) & 0.195 & 0.170 & 0.176 & 0.107 & 24.371 \\
\bottomrule
\end{tabular}
\caption{Tiempo de entrenamiento y uso máximo de VRAM por modelo. Todos los modelos fueron entrenados en GPU y utilizando el mismo entorno.}
\label{tab:hardware_tiempo}
\end{table}

La Tabla~\ref{tab:hardware_tiempo} revela una dicotomía marcada entre modelos ligeros y el Transformer preentrenado. Los modelos recurrentes con $\sim$1.26–1.95M parámetros requieren recursos modestos ($<$0.2 GB VRAM), aunque su eficiencia temporal diverge significativamente: RNN completa en 0.42 min gracias a su simplicidad arquitectónica (sin compuertas), mientras LSTM y GRU demandan $\sim$3 min debido a operaciones secuenciales no paralelizables en sus mecanismos de compuerta. CNN destaca como una arquitectura eficiente en ambas dimensiones al explotar el paralelismo de las convoluciones con solo 1.18M parámetros y operaciones independientes sobre n-gramas. En contraste, mDeBERTa-v3 representa un salto cuantitativo: 24.37 GB VRAM (distribuidos en 2 GPUs) y 14.71 min reflejan su escala de 280M parámetros, atención multi-cabeza desentrelazada en 12 capas, y fine-tuning completo con FP16. El factor de escalamiento hardware es $\sim$125× en VRAM y $\sim$26× en tiempo respecto a CNN podría ser justificado por la ganancia de +10.2 puntos F1, dependiendo del caso de uso.




\newpage
\bibliographystyle{unsrt} % Elige un estilo (otros: abbrvnat, unsrtnat, etc.)
\bibliography{bib} % Indica el nombre de tu archivo .bib (sin la extensión)


\end{document}
